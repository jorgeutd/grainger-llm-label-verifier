{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ceba958f8bc64f6a9a652ff7919762b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_340534e2773f4761900e8de3e7681e42",
              "IPY_MODEL_816465ce9ff3478b87f3debb8b4903ce",
              "IPY_MODEL_0409aa4938b645b98b0e4822e4d187cf"
            ],
            "layout": "IPY_MODEL_ddc147cbc71c4455a1c90871ba6edb2a"
          }
        },
        "340534e2773f4761900e8de3e7681e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0dc1b526d45417cadeb658fc0014e39",
            "placeholder": "​",
            "style": "IPY_MODEL_5b8adbb383414d04a1e63d4cd85a9ace",
            "value": "Processing Items: 100%"
          }
        },
        "816465ce9ff3478b87f3debb8b4903ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0779ff545942461ab7024ad015cd3c3e",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_479616ab43094cf195a09adff62a69f3",
            "value": 24
          }
        },
        "0409aa4938b645b98b0e4822e4d187cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_043e596aae604f7cb85efe469697fade",
            "placeholder": "​",
            "style": "IPY_MODEL_78381d28c4064ccebc884b55354f1a81",
            "value": " 24/24 [01:52&lt;00:00,  3.75s/it]"
          }
        },
        "ddc147cbc71c4455a1c90871ba6edb2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0dc1b526d45417cadeb658fc0014e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b8adbb383414d04a1e63d4cd85a9ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0779ff545942461ab7024ad015cd3c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "479616ab43094cf195a09adff62a69f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "043e596aae604f7cb85efe469697fade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78381d28c4064ccebc884b55354f1a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v9zGygGhgXss"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph langchain python-dotenv langchain-anthropic pandas pyarrow pydantic -q -U\n",
        "!apt-get install -qq graphviz libgraphviz-dev # Install system library quietly\n",
        "!pip install -q pygraphviz # Install Python wrapper quietly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "01_llm_verification_reflection_agent.ipynb\n",
        "\n",
        "Alternative solution using a LangGraph Reflection Agent pattern\n",
        "for the Grainger LLM Verification task, using separate Generator and Critic models.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DJjH8WkJnTYN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8257053e-d98a-48d5-c63b-27e9312ea5f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n01_llm_verification_reflection_agent.ipynb\\n\\nAlternative solution using a LangGraph Reflection Agent pattern\\nfor the Grainger LLM Verification task, using separate Generator and Critic models.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Environment Setup ---\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np # Import numpy for NA checks if needed\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "import warnings\n",
        "from typing import TypedDict, List, Optional, Annotated, Dict, Any\n",
        "from IPython.display import Image, display\n",
        "import logging\n",
        "\n",
        "# LangChain & LangGraph imports\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field # Using v1 for broader compatibility\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# --- Load Environment Variables ---\n",
        "## --- Load Environment Variables using Colab Secrets ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "    if not ANTHROPIC_API_KEY:\n",
        "        raise ValueError(\"ANTHROPIC_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "    else:\n",
        "        print(\"ANTHROPIC_API_KEY loaded successfully from Colab Secrets.\")\n",
        "    # No need to set os.environ explicitly if passing the key directly to the client later\n",
        "except ImportError:\n",
        "    print(\"Could not import google.colab.userdata. Are you running in Colab?\")\n",
        "    # Fallback or error handling if not in Colab\n",
        "    raise EnvironmentError(\"Not running in Colab and Colab Secrets unavailable.\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# LLM Initialization\n",
        "GENERATOR_MODEL_NAME = \"claude-3-5-sonnet-20241022\" #\n",
        "CRITIC_MODEL_NAME = \"claude-3-7-sonnet-latest\"\n",
        "\n",
        "try:\n",
        "    # Pass the key explicitly for clarity and robustness\n",
        "    generator_llm = ChatAnthropic(\n",
        "        model=GENERATOR_MODEL_NAME,\n",
        "        temperature=0.1,\n",
        "        anthropic_api_key=ANTHROPIC_API_KEY # Pass the loaded key\n",
        "    )\n",
        "    print(f\"Generator LLM initialized: {GENERATOR_MODEL_NAME}\")\n",
        "\n",
        "    critic_llm = ChatAnthropic(\n",
        "        model=CRITIC_MODEL_NAME,\n",
        "        temperature=0.0,\n",
        "        anthropic_api_key=ANTHROPIC_API_KEY # Pass the loaded key\n",
        "    )\n",
        "    print(f\"Critic LLM initialized: {CRITIC_MODEL_NAME}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Anthropic models: {e}\")\n",
        "    raise\n",
        "\n",
        "# .configuration: Data Path, MAX_ITERATIONS, Output file\n",
        "PROCESSED_DATA_PATH = '/content/drive/MyDrive/Colab_Data/Grainger_LLM_Exercise/filtered_data_with_context_2025-04-22.parquet'\n",
        "MAX_ITERATIONS = 3\n",
        "REFLECTION_RESULTS_CSV = \"results/grainger_llm_reflection_results_multi_model_04_23.csv\"\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "print(\"Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuwM1jk_gzY5",
        "outputId": "2459cb2f-338b-4084-9cbe-9cbe1ee92425"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANTHROPIC_API_KEY loaded successfully from Colab Secrets.\n",
            "Generator LLM initialized: claude-3-5-sonnet-20241022\n",
            "Critic LLM initialized: claude-3-7-sonnet-latest\n",
            "Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "try:\n",
        "    # Add force_remount=True\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise SystemExit(\"Google Drive mount failed. Cannot proceed.\")\n",
        "\n",
        "# Optional: Verify the file exists after mounting\n",
        "import os\n",
        "# Use the same PROCESSED_DATA_PATH variable\n",
        "if os.path.exists(PROCESSED_DATA_PATH):\n",
        "    print(f\"Verified: File exists at {PROCESSED_DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"WARNING: File NOT found at {PROCESSED_DATA_PATH} after mounting.\")\n",
        "    print(\"Please double-check the path and filename in your Google Drive.\")\n",
        "    # List contents of the expected directory to help debug\n",
        "    expected_dir = os.path.dirname(PROCESSED_DATA_PATH)\n",
        "    if os.path.exists(expected_dir):\n",
        "         print(f\"Contents of {expected_dir}:\")\n",
        "         try:\n",
        "             print(os.listdir(expected_dir))\n",
        "         except Exception as list_e:\n",
        "             print(f\"Could not list directory contents: {list_e}\")\n",
        "    else:\n",
        "         print(f\"Directory {expected_dir} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TcxxB1noItX",
        "outputId": "eefa7cc3-bb1c-4556-bbce-897a5f9f1133"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Verified: File exists at /content/drive/MyDrive/Colab_Data/Grainger_LLM_Exercise/filtered_data_with_context_2025-04-22.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Preprocessed Data from end to end worfklow notebook  step 6.6---\n",
        "try:\n",
        "    # Ensure the path is correct for your environment (Colab Drive vs local)\n",
        "    df_input = pd.read_parquet(PROCESSED_DATA_PATH)\n",
        "    print(f\"Loaded data from {PROCESSED_DATA_PATH}. Shape: {df_input.shape}\")\n",
        "    # Ensure required columns are present\n",
        "    required_cols = ['query_id', 'product_id', 'query', 'llm_product_context']\n",
        "    if not all(col in df_input.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df_input.columns]\n",
        "        raise ValueError(f\"Missing required columns in input data: {missing}\")\n",
        "    if df_input.empty:\n",
        "         raise ValueError(\"Input DataFrame is empty.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Processed data file not found at {PROCESSED_DATA_PATH}\")\n",
        "    print(\"Please ensure the file exists or run the data preparation steps first.\")\n",
        "    # Handle error appropriately, e.g., raise SystemExit or run data prep\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "\n",
        "# --- Pydantic Model for Structured Output ---\n",
        "class VerificationResult(BaseModel):\n",
        "    \"\"\"Structured output for the verification task.\"\"\"\n",
        "    is_exact_match: bool = Field(description=\"True if the product is an exact match based on the rules, False otherwise.\")\n",
        "    reasoning: str = Field(description=\"Concise explanation citing the specific rule and evidence (contradiction or confirmation/missing info). Max 1-2 sentences.\")\n",
        "    reformulated_query: Optional[str] = Field(description=\"If is_exact_match is False, the corrected query. Must be null if is_exact_match is True.\")\n",
        "\n",
        "# --- Prompts ---\n",
        "VERIFIER_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an extremely precise AI Data Quality Analyst. Your task is to verify if the Product Information makes the product an \"Exact\" match for the Search Query based *only* on the provided text and strict rules.\n",
        "\n",
        "**Definition of Exact Match ('E'):**\n",
        "The Product is relevant for the Query AND satisfies ALL specifications mentioned in the Query.\n",
        "\n",
        "**Crucial Rules:**\n",
        "1.  **Contradiction Rule:** If Product Info EXPLICITLY CONTRADICTS a Query specification (e.g., query \"100 pack\", product \"50 count\"; query \"without shams\", product \"includes shams\"), it's NOT Exact. Mark `is_exact_match: false`.\n",
        "2.  **Missing Information Rule:** If Product Info DOES NOT MENTION a query spec (e.g., query \"gyroscopic\", feature not mentioned), assume it MIGHT satisfy it. DO NOT mark as non-Exact based *only* on missing info. It remains an Exact candidate (`is_exact_match: true` unless Rule 1 applies).\n",
        "3.  **Extra Information Rule:** Extra product details NOT in the query are acceptable if Rule 1 isn't violated.\n",
        "\n",
        "**Input:**\n",
        "Search Query: \"{query}\"\n",
        "Product Information:\n",
        "--- START ---\n",
        "{product_context}\n",
        "--- END ---\n",
        "\n",
        "**Output:**\n",
        "Respond ONLY with a valid JSON object adhering to the VerificationResult schema. Provide clear reasoning based *directly* on the rules and text. If false, formulate a concise, accurate query based on the product details provided. If true, reformulated_query MUST be null.\"\"\"),\n",
        "    (\"human\", \"Analyze the provided query and product information according to the rules and provide your assessment in the required JSON format.\")\n",
        "])\n",
        "\n",
        "REFLECTOR_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a meticulous Quality Assurance expert evaluating an AI's verification judgment. Your goal is to ensure the AI strictly followed the rules and produced a correct, well-reasoned assessment.\n",
        "\n",
        "**Evaluation Context:**\n",
        "Original Search Query: \"{query}\"\n",
        "Product Information:\n",
        "--- START ---\n",
        "{product_context}\n",
        "--- END ---\n",
        "\n",
        "AI Verifier's Assessment (JSON):\n",
        "```json\n",
        "{assessment_json}\n",
        "\n",
        "Your Task:\n",
        "Critically evaluate the AI Verifier's Assessment based only on the provided Query, Product Information, and these Crucial Rules:\n",
        "Contradiction Rule: Was any EXPLICIT CONTRADICTION between Query and Product Info correctly identified (leading to is_exact_match: false)? Or was a contradiction missed?\n",
        "Missing Information Rule: Was the rule applied correctly? Did the AI correctly determine is_exact_match: true when information was merely missing (and no contradiction existed)? Or did it incorrectly flag missing info as a reason for is_exact_match: false?\n",
        "Reasoning Quality: Is the reasoning accurate, concise, and directly supported by the text and rules? Does it clearly state the basis (contradiction found, or spec met/missing)?\n",
        "Reformulation Quality: If is_exact_match: false, is the reformulated_query accurate based on the product info, relevant to the original query's intent, and concise? Is it correctly null if is_exact_match: true?\n",
        "JSON Validity: Is the assessment a valid JSON matching the required schema? (Assume basic validity if parsed).\n",
        "\n",
        "Output:\n",
        "If the AI's Assessment is PERFECT and strictly adheres to all rules: Respond ONLY with the word ACCEPT.\n",
        "If there are ANY flaws (rule misapplication, poor reasoning, incorrect reformulation, wrong null handling): Provide a concise, actionable critique (1-3 sentences) explaining EXACTLY what needs to be fixed. Focus on the most critical error if multiple exist. Do NOT provide the corrected JSON yourself.\"\"\"),\n",
        "(\"human\", \"Evaluate the AI Verifier's Assessment based on the rules and context. Provide your critique or 'ACCEPT'.\")\n",
        "])\n",
        "\n",
        "\n",
        "REVISER_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
        "(\"system\", \"\"\"You are an AI Data Quality Analyst revising your previous assessment based on critique.\n",
        "\n",
        "Original Task & Rules (Reminder):\n",
        "Verify if the Product Information makes the product an \"Exact\" match for the Search Query based on the rules below. Output valid JSON.\n",
        "Definition Exact ('E'): Relevant AND satisfies ALL query specs.\n",
        "Rule 1 (Contradiction): Explicit contradiction -> is_exact_match: false.\n",
        "Rule 2 (Missing Info): Spec not mentioned -> Assume OK (is_exact_match: true unless Rule 1 applies).\n",
        "Rule 3 (Extra Info): Extra product details OK if no contradiction.\n",
        "Context:\n",
        "Search Query: \"{query}\"\n",
        "Product Information:\n",
        "--- START ---\n",
        "{product_context}\n",
        "--- END ---\n",
        "Your Previous Assessment (JSON):\n",
        "{previous_assessment_json}\n",
        "\n",
        "Critique Received:\n",
        "\"{critique}\"\n",
        "\n",
        "Your Task:\n",
        "Carefully consider the Critique. Re-evaluate the Query and Product Information based only on the original Rules and the specific points raised in the Critique.\n",
        "Generate a revised, valid JSON object adhering to the VerificationResult schema, addressing the critique accurately. Ensure reformulated_query is null if is_exact_match is true.\"\"\"),\n",
        "(\"human\", \"Revise your assessment based on the critique and rules. Provide the updated JSON output.\")\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wocK6lhQmG6Y",
        "outputId": "1066e81d-1aff-4cda-9572-8095472629b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data from /content/drive/MyDrive/Colab_Data/Grainger_LLM_Exercise/filtered_data_with_context_2025-04-22.parquet. Shape: (24, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bind the GENERATOR LLM for structured output ---\n",
        "# Ensure generator_llm is defined from the setup block\n",
        "structured_llm_generator = generator_llm.with_structured_output(VerificationResult)\n",
        "\n",
        "print(\"Core Components Defined (Pydantic Model, Prompts, Structured LLM binding).\")\n",
        "\n",
        "\n",
        "# --- LangGraph State Definition ---\n",
        "# The state defines the data that flows through the graph.\n",
        "# Each node updates parts of this state.\n",
        "class VerificationState(TypedDict):\n",
        "    query: str                      # Input: The user's search query\n",
        "    product_context: str            # Input: The aggregated product information\n",
        "    assessment: Optional[VerificationResult] # Output of verify/revise: The current JSON assessment\n",
        "    critique: Optional[str]         # Output of reflect: The critique text or \"ACCEPT\"\n",
        "    revision_number: int            # Internal counter for reflection cycles\n",
        "    error: Optional[str]            # Captures errors from nodes\n",
        "\n",
        "\n",
        "# --- Graph Node Definitions ---\n",
        "# Nodes are functions that perform actions and update the state.\n",
        "\n",
        "def initial_verify(state: VerificationState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Node: Performs the first verification attempt.\n",
        "    Input: query, product_context\n",
        "    Output: assessment, error, revision_number (incremented)\n",
        "    LLM Used: Generator (structured output)\n",
        "    \"\"\"\n",
        "    print(f\"--- Iteration {state['revision_number']}: Initial Verification (using {GENERATOR_MODEL_NAME}) ---\")\n",
        "    query = state['query']\n",
        "    product_context = state['product_context']\n",
        "    error = None\n",
        "    assessment = None\n",
        "    try:\n",
        "        prompt = VERIFIER_PROMPT_TEMPLATE.format(query=query, product_context=product_context)\n",
        "        assessment = structured_llm_generator.invoke(prompt) # Call LLM for structured JSON\n",
        "        print(f\"Initial Assessment: {assessment.json(indent=2)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in initial_verify: {e}\")\n",
        "        error = f\"Failed during initial verification: {e}\"\n",
        "    # Increment revision number for the next potential cycle\n",
        "    return {\"assessment\": assessment, \"error\": error, \"revision_number\": state.get(\"revision_number\", 0) + 1}\n",
        "\n",
        "def reflect(state: VerificationState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Node: Critiques the current assessment.\n",
        "    Input: query, product_context, assessment, error (from previous step)\n",
        "    Output: critique, error\n",
        "    LLM Used: Critic\n",
        "    \"\"\"\n",
        "    current_revision = state.get(\"revision_number\", 1) # Revision number *after* verify/revise\n",
        "    print(f\"--- Iteration {current_revision - 1}: Reflection (using {CRITIC_MODEL_NAME}) ---\")\n",
        "    if state.get(\"error\"): # If verify/revise failed, skip reflection\n",
        "        print(\"Skipping reflection due to previous error.\")\n",
        "        return {\"critique\": \"Previous step failed\", \"error\": state.get(\"error\")}\n",
        "\n",
        "    query = state['query']\n",
        "    product_context = state['product_context']\n",
        "    assessment = state['assessment']\n",
        "    critique = None\n",
        "    error = None\n",
        "\n",
        "    if not assessment: # Should not happen if verify worked, but check defensively\n",
        "        error = \"Cannot reflect: No assessment provided from previous step.\"\n",
        "        print(error)\n",
        "        return {\"critique\": None, \"error\": error}\n",
        "\n",
        "    try:\n",
        "        assessment_json_str = assessment.json()\n",
        "        prompt = REFLECTOR_PROMPT_TEMPLATE.format(\n",
        "            query=query,\n",
        "            product_context=product_context,\n",
        "            assessment_json=assessment_json_str\n",
        "        )\n",
        "        response = critic_llm.invoke(prompt) # Use the critic LLM\n",
        "        critique = response.content.strip()\n",
        "        print(f\"Reflection Critique: {critique}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in reflect: {e}\")\n",
        "        error = f\"Failed during reflection: {e}\"\n",
        "\n",
        "    # Standardize the acceptance signal\n",
        "    if critique and critique.upper() == \"ACCEPT\":\n",
        "        critique = \"ACCEPT\"\n",
        "\n",
        "    # Return the critique; revision number doesn't change in this step's output\n",
        "    return {\"critique\": critique, \"error\": error}\n",
        "\n",
        "\n",
        "def revise(state: VerificationState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Node: Revises the assessment based on the critique.\n",
        "    Input: query, product_context, assessment, critique, revision_number, error\n",
        "    Output: assessment (updated), error, revision_number (incremented)\n",
        "    LLM Used: Generator (structured output)\n",
        "    \"\"\"\n",
        "    current_revision = state.get(\"revision_number\", 1) # Revision number *before* this revision\n",
        "    print(f\"--- Iteration {current_revision - 1}: Revision (using {GENERATOR_MODEL_NAME}) ---\")\n",
        "    if state.get(\"error\"): # If reflection failed, skip revision\n",
        "        print(\"Skipping revision due to previous error.\")\n",
        "        # Pass along previous state's assessment and error\n",
        "        return {\"assessment\": state.get(\"assessment\"), \"error\": state.get(\"error\"), \"revision_number\": current_revision}\n",
        "\n",
        "    query = state['query']\n",
        "    product_context = state['product_context']\n",
        "    previous_assessment = state['assessment']\n",
        "    critique = state['critique']\n",
        "    revised_assessment = None\n",
        "    error = None\n",
        "\n",
        "    # This node should only run if critique is NOT \"ACCEPT\"\n",
        "    if not previous_assessment or not critique or critique == \"ACCEPT\":\n",
        "        error = \"Revise node called inappropriately (no assessment/critique or critique was ACCEPT).\"\n",
        "        print(f\"WARNING: {error}\")\n",
        "        # Return previous state without incrementing revision number if called incorrectly\n",
        "        return {\"assessment\": previous_assessment, \"error\": error, \"revision_number\": current_revision}\n",
        "\n",
        "    try:\n",
        "        previous_assessment_json_str = previous_assessment.json()\n",
        "        prompt = REVISER_PROMPT_TEMPLATE.format(\n",
        "            query=query,\n",
        "            product_context=product_context,\n",
        "            previous_assessment_json=previous_assessment_json_str,\n",
        "            critique=critique\n",
        "        )\n",
        "        revised_assessment = structured_llm_generator.invoke(prompt) # Use generator for structured output\n",
        "        print(f\"Revised Assessment: {revised_assessment.json(indent=2)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in revise: {e}\")\n",
        "        error = f\"Failed during revision: {e}\"\n",
        "        # Keep the previous assessment if revision fails to avoid losing progress\n",
        "        revised_assessment = previous_assessment\n",
        "\n",
        "    # Return the revised assessment and increment revision number for the next cycle\n",
        "    return {\"assessment\": revised_assessment, \"error\": error, \"revision_number\": current_revision + 1}\n",
        "\n",
        "\n",
        "# --- Conditional Edge Logic ---\n",
        "# This function directs the flow of the graph after the 'reflect' node runs.\n",
        "def decide_to_finish(state: VerificationState) -> str:\n",
        "    \"\"\"\n",
        "    Determines the next step after reflection: revise or end the process.\n",
        "    Input: The current state after the 'reflect' node.\n",
        "    Output: A string representing the next node ('revise') or the end state (END).\n",
        "    \"\"\"\n",
        "    error = state.get(\"error\")\n",
        "    critique = state.get(\"critique\")\n",
        "    # revision_number reflects the *upcoming* cycle attempt number (e.g., 1 after initial verify)\n",
        "    revision_number = state.get(\"revision_number\", 0)\n",
        "\n",
        "    print(f\"Decision Check: Error='{error}', Critique='{critique}', Revision Cycle Attempt='{revision_number}'\")\n",
        "\n",
        "    # Prioritize ending on critical errors from verify or reflect\n",
        "    is_critical_error = error and \"Revise node called inappropriately\" not in error\n",
        "\n",
        "    if is_critical_error:\n",
        "        print(f\"Deciding to finish due to critical error: {error}\")\n",
        "        return END # Stop the graph execution for this item\n",
        "    elif critique == \"ACCEPT\":\n",
        "        print(\"Deciding to finish: Reflection accepted.\")\n",
        "        return END # Stop the graph, the assessment is good\n",
        "    elif revision_number > MAX_ITERATIONS: # Have we exceeded the allowed cycles?\n",
        "        print(f\"Deciding to finish: Max iterations ({MAX_ITERATIONS}) reached.\")\n",
        "        return END # Stop the graph due to hitting the limit\n",
        "    else:\n",
        "        # If no error, not accepted, and within iteration limit, proceed to revise\n",
        "        print(\"Deciding to revise.\")\n",
        "        return \"revise\" # Route to the 'revise' node\n",
        "\n",
        "# --- Build the Graph ---\n",
        "# We are building the reflection logic manually using LangGraph's core components.\n",
        "# We are *not* using the separate `langgraph-reflection` helper library here.\n",
        "# This gives us more control over the specific flow and state management.\n",
        "\n",
        "workflow = StateGraph(VerificationState)\n",
        "\n",
        "# Add the defined functions as nodes in the graph\n",
        "workflow.add_node(\"verify\", initial_verify)\n",
        "workflow.add_node(\"reflect\", reflect)\n",
        "workflow.add_node(\"revise\", revise)\n",
        "\n",
        "# Define the starting point of the graph\n",
        "workflow.set_entry_point(\"verify\")\n",
        "\n",
        "# Define the connections (edges) between nodes\n",
        "workflow.add_edge(\"verify\", \"reflect\") # Always go from initial verification to reflection\n",
        "\n",
        "# Add conditional logic after the 'reflect' node\n",
        "workflow.add_conditional_edges(\n",
        "    \"reflect\",          # The source node\n",
        "    decide_to_finish,   # The function that decides the next step\n",
        "    {\n",
        "        \"revise\": \"revise\", # If decide_to_finish returns \"revise\", go to the 'revise' node\n",
        "        END: END            # If decide_to_finish returns END, terminate the graph for this run\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define the loop: after revising, go back to reflect on the revision\n",
        "workflow.add_edge(\"revise\", \"reflect\")\n",
        "\n",
        "# Compile the graph into a runnable application object\n",
        "app = workflow.compile()\n",
        "print(\"LangGraph Application Compiled.\")\n",
        "\n",
        "# --- Visualize the graph ---\n",
        "print(\"\\nAttempting to visualize the graph...\")\n",
        "try:\n",
        "    img_data = app.get_graph().draw_mermaid_png() # Generate the graph image\n",
        "    if img_data:\n",
        "        print(\"Displaying Mermaid PNG:\")\n",
        "        display(Image(img_data)) # Display in the notebook\n",
        "    else:\n",
        "        # Fallback if PNG generation fails but no error is raised\n",
        "        print(\"draw_mermaid_png() did not return image data. Graphviz might have issues.\")\n",
        "        print(\"Attempting fallback with draw_ascii()...\")\n",
        "        try:\n",
        "            ascii_graph = app.get_graph().draw_ascii()\n",
        "            print(\"\\nASCII Representation of the Graph:\")\n",
        "            print(ascii_graph)\n",
        "        except Exception as ascii_e:\n",
        "            print(f\"Could not draw ASCII graph either: {ascii_e}\")\n",
        "\n",
        "except ImportError as e:\n",
        "    # Catch if pygraphviz is missing\n",
        "    print(f\"ImportError: {e}. Visualization requires pygraphviz and graphviz system library.\")\n",
        "    print(\"Install with: !apt-get install -qq graphviz libgraphviz-dev && pip install -q pygraphviz\")\n",
        "except Exception as e:\n",
        "    # Catch other errors during visualization\n",
        "    print(f\"Could not draw graph: {e}\")\n",
        "    print(\"Attempting fallback with draw_ascii()...\")\n",
        "    try:\n",
        "        ascii_graph = app.get_graph().draw_ascii()\n",
        "        print(\"\\nASCII Representation of the Graph:\")\n",
        "        print(ascii_graph)\n",
        "    except Exception as ascii_e:\n",
        "        print(f\"Could not draw ASCII graph either: {ascii_e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ddLJcQPtrG2u",
        "outputId": "e9cb14d0-7f5c-4677-ea54-cb22f7127926"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core Components Defined (Pydantic Model, Prompts, Structured LLM binding).\n",
            "LangGraph Application Compiled.\n",
            "\n",
            "Attempting to visualize the graph...\n",
            "Displaying Mermaid PNG:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the Workflow for Each Row ---\n",
        "results_list = []\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import numpy as np # Import numpy for NA checks if needed\n",
        "import pandas as pd # Ensure pandas is imported if running this cell separately\n",
        "\n",
        "print(f\"\\nStarting Reflection Agent Workflow for {len(df_input)} items...\")\n",
        "\n",
        "start_overall_time = time.time()\n",
        "\n",
        "# Iterate through each row of the prepared DataFrame\n",
        "for index, row in tqdm(df_input.iterrows(), total=len(df_input), desc=\"Processing Items\"):\n",
        "    query_id = row['query_id']\n",
        "    product_id = row['product_id']\n",
        "    query = row['query']\n",
        "    # Handle potential missing context robustly\n",
        "    context = row.get('llm_product_context', \"Product information missing.\")\n",
        "    if pd.isna(context) or not context:\n",
        "         context = \"Product information missing.\"\n",
        "\n",
        "    print(f\"\\n===== Processing Item {index+1}/{len(df_input)} (PID: {product_id}) =====\")\n",
        "    # Define the initial state for this item's graph run\n",
        "    initial_state = VerificationState(\n",
        "        query=query,\n",
        "        product_context=context,\n",
        "        assessment=None,\n",
        "        critique=None,\n",
        "        revision_number=0, # Start revision count at 0 (will become 1 after initial_verify)\n",
        "        error=None,\n",
        "    )\n",
        "\n",
        "    final_state = None\n",
        "    # Initialize a dictionary to store the results for this specific item\n",
        "    item_result = {\n",
        "        'query_id': query_id,\n",
        "        'product_id': product_id,\n",
        "        'accurate_label': None, # Default to None (will be updated based on final assessment)\n",
        "        'reformulated_query': None, # Default to None\n",
        "        'final_reasoning': \"Workflow did not produce a final assessment.\", # Default message\n",
        "        'iterations': 0, # Number of revision cycles (0 means only initial verify + reflect)\n",
        "        'final_outcome': \"Unknown\" # Tracks how the graph ended (Accepted, Error, Max Iterations)\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Execute the compiled LangGraph application for this item's initial state\n",
        "        # The recursion_limit prevents infinite loops if something goes wrong\n",
        "        final_state = app.invoke(initial_state, {\"recursion_limit\": MAX_ITERATIONS * 2 + 5})\n",
        "\n",
        "        # Process the final state returned by the graph after it reaches END\n",
        "        if final_state:\n",
        "            # Calculate the number of revision cycles completed\n",
        "            # revision_number in the final state is the number of the *next* cycle attempt\n",
        "            item_result['iterations'] = final_state.get('revision_number', 1) - 1 # Iteration 0 is initial verify\n",
        "\n",
        "            # Extract key information from the final state\n",
        "            final_assessment = final_state.get('assessment')\n",
        "            final_critique = final_state.get('critique') # The critique that led to the END state\n",
        "            final_error = final_state.get('error') # Any error that occurred\n",
        "\n",
        "            # Determine the final outcome and extract results based on how the graph ended\n",
        "            is_critical_error = final_error and \"Revise node called inappropriately\" not in final_error\n",
        "            max_iters_reached = item_result['iterations'] >= MAX_ITERATIONS\n",
        "\n",
        "            if is_critical_error:\n",
        "                item_result['final_outcome'] = \"Error\"\n",
        "                item_result['final_reasoning'] = f\"Workflow ended with error: {final_error}\"\n",
        "                # accurate_label remains None\n",
        "            elif final_critique == \"ACCEPT\":\n",
        "                item_result['final_outcome'] = \"Accepted\"\n",
        "                # Extract results from the accepted assessment\n",
        "                if final_assessment and isinstance(final_assessment, VerificationResult):\n",
        "                    item_result['accurate_label'] = final_assessment.is_exact_match\n",
        "                    item_result['reformulated_query'] = final_assessment.reformulated_query\n",
        "                    item_result['final_reasoning'] = final_assessment.reasoning\n",
        "                else:\n",
        "                     # If accepted but assessment is somehow invalid, flag it\n",
        "                     item_result['final_reasoning'] = \"Accepted, but final assessment missing or invalid.\"\n",
        "                     item_result['accurate_label'] = None # Mark as undetermined\n",
        "            elif max_iters_reached:\n",
        "                item_result['final_outcome'] = \"Max Iterations Reached\"\n",
        "                # Take the last assessment available when max iterations is hit\n",
        "                if final_assessment and isinstance(final_assessment, VerificationResult):\n",
        "                    item_result['accurate_label'] = final_assessment.is_exact_match\n",
        "                    item_result['reformulated_query'] = final_assessment.reformulated_query\n",
        "                    item_result['final_reasoning'] = final_assessment.reasoning + \" [Note: Max iterations reached]\"\n",
        "                else:\n",
        "                     item_result['final_reasoning'] = \"Max iterations reached, but final assessment missing or invalid.\"\n",
        "                     item_result['accurate_label'] = None # Mark as undetermined\n",
        "            else:\n",
        "                # Defensive case: Graph ended without ACCEPT, Error, or Max Iterations\n",
        "                item_result['final_outcome'] = \"Ended Unexpectedly\"\n",
        "                item_result['final_reasoning'] = f\"Ended unexpectedly. Last critique: {final_critique}. Last Error: {final_error}\"\n",
        "                item_result['accurate_label'] = None # Mark as undetermined\n",
        "\n",
        "        else:\n",
        "            # If app.invoke returns None (shouldn't typically happen unless graph is ill-defined)\n",
        "            item_result['final_outcome'] = \"Error\"\n",
        "            item_result['final_reasoning'] = \"Graph invocation returned None.\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch errors occurring outside the graph execution itself (e.g., during setup for the loop)\n",
        "        print(f\"\\nFATAL ERROR processing item index {index} (PID: {product_id}): {e}\")\n",
        "        item_result['final_reasoning'] = f\"Outer loop error: {e}\"\n",
        "        item_result['final_outcome'] = \"Outer Loop Error\"\n",
        "        item_result['accurate_label'] = None # Ensure label is None on outer error\n",
        "        # Optional: Add traceback for debugging\n",
        "        # import traceback\n",
        "        # item_result['error_trace'] = traceback.format_exc()\n",
        "\n",
        "    # Final check to ensure accurate_label is valid (boolean or None/NA)\n",
        "    if item_result['accurate_label'] not in [True, False, None, pd.NA]:\n",
        "         warnings.warn(f\"Item {index} resulted in non-boolean/None accurate_label: {item_result['accurate_label']}. Setting to None.\")\n",
        "         item_result['accurate_label'] = None\n",
        "\n",
        "\n",
        "    # Append the processed result for this item to the list\n",
        "    results_list.append(item_result)\n",
        "    print(f\"===== Finished Item {index+1} (PID: {product_id}) - Outcome: {item_result['final_outcome']} =====\")\n",
        "\n",
        "\n",
        "end_overall_time = time.time()\n",
        "print(f\"\\nReflection Agent Workflow Completed in {end_overall_time - start_overall_time:.2f} seconds.\")\n",
        "\n",
        "# --- Process and Display Results ---\n",
        "\n",
        "# Convert the list of result dictionaries into a pandas DataFrame\n",
        "df_results_reflection = pd.DataFrame(results_list)\n",
        "\n",
        "# Explicitly set dtype for boolean column to handle potential NAs correctly (uses pandas nullable boolean type)\n",
        "df_results_reflection['accurate_label'] = df_results_reflection['accurate_label'].astype('boolean')\n",
        "\n",
        "print(\"\\n--- Sample Results (Reflection Agent - Multi-Model) ---\")\n",
        "# Display the first few rows of the results DataFrame\n",
        "with pd.option_context('display.max_rows', 10, 'display.max_colwidth', 150):\n",
        "    display(df_results_reflection.head())\n",
        "\n",
        "# --- Basic Statistics ---\n",
        "print(\"\\n--- Basic Stats ---\")\n",
        "print(f\"Total items processed: {len(df_results_reflection)}\")\n",
        "\n",
        "# Analyze the distribution of the final 'accurate_label'\n",
        "if 'accurate_label' in df_results_reflection.columns:\n",
        "    print(\"\\nFinal Label Distribution:\")\n",
        "    # Use value_counts with dropna=False to include undetermined cases (None/pd.NA)\n",
        "    label_counts = df_results_reflection['accurate_label'].value_counts(dropna=False).rename({True: 'Accurate', False: 'Inaccurate'})\n",
        "    # Rename the index for NaN/NA to be more descriptive\n",
        "    if pd.isna(label_counts.index).any():\n",
        "        rename_map = {idx: 'Undetermined (Error/MaxIter)' for idx in label_counts.index if pd.isna(idx)}\n",
        "        label_counts = label_counts.rename(index=rename_map)\n",
        "    print(label_counts)\n",
        "else:\n",
        "    print(\"Accurate label column not found.\")\n",
        "\n",
        "# Analyze how many revision iterations were needed\n",
        "if 'iterations' in df_results_reflection.columns:\n",
        "    print(\"\\nIterations Distribution (0 = initial verify only):\")\n",
        "    print(df_results_reflection['iterations'].value_counts().sort_index())\n",
        "\n",
        "# Analyze how the graph execution ended for each item\n",
        "if 'final_outcome' in df_results_reflection.columns:\n",
        "    print(\"\\nFinal Outcome Distribution:\")\n",
        "    print(df_results_reflection['final_outcome'].value_counts())\n",
        "\n",
        "\n",
        "# --- Save Final Results ---\n",
        "# Define the columns required for the final deliverable CSV\n",
        "submission_columns = ['query_id', 'product_id', 'accurate_label', 'reformulated_query']\n",
        "\n",
        "try:\n",
        "    # Ensure the final DataFrame has the necessary columns\n",
        "    if not all(col in df_results_reflection.columns for col in submission_columns):\n",
        "        missing_sub = [col for col in submission_columns if col not in df_results_reflection.columns]\n",
        "        raise ValueError(f\"Cannot save submission file. Missing columns: {missing_sub}\")\n",
        "\n",
        "    # Create the DataFrame for submission, selecting only required columns described on the PDF document\n",
        "    submission_df = df_results_reflection[submission_columns].copy()\n",
        "\n",
        "    # Clean up for CSV: Ensure reformulated_query is an empty string instead of None/NA\n",
        "    submission_df['reformulated_query'] = submission_df['reformulated_query'].fillna('').astype(str)\n",
        "    # Note: Pandas to_csv usually handles None/pd.NA in boolean columns as empty strings by default.\n",
        "\n",
        "    # Save the submission file\n",
        "    submission_df.to_csv(REFLECTION_RESULTS_CSV, index=False, encoding='utf-8')\n",
        "    print(f\"\\nSuccessfully saved reflection agent results to: {REFLECTION_RESULTS_CSV}\")\n",
        "\n",
        "    # let's save also  the full results DataFrame (including iterations, outcome, reasoning) for analysis\n",
        "    full_reflection_results_csv = REFLECTION_RESULTS_CSV.replace(\".csv\", \"_FULL.csv\")\n",
        "    df_results_reflection.to_csv(full_reflection_results_csv, index=False, encoding='utf-8')\n",
        "    print(f\"Successfully saved full reflection agent results to: {full_reflection_results_csv}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR saving reflection results: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ceba958f8bc64f6a9a652ff7919762b9",
            "340534e2773f4761900e8de3e7681e42",
            "816465ce9ff3478b87f3debb8b4903ce",
            "0409aa4938b645b98b0e4822e4d187cf",
            "ddc147cbc71c4455a1c90871ba6edb2a",
            "c0dc1b526d45417cadeb658fc0014e39",
            "5b8adbb383414d04a1e63d4cd85a9ace",
            "0779ff545942461ab7024ad015cd3c3e",
            "479616ab43094cf195a09adff62a69f3",
            "043e596aae604f7cb85efe469697fade",
            "78381d28c4064ccebc884b55354f1a81"
          ]
        },
        "id": "JXaCjd7nrMpY",
        "outputId": "aaa8896c-041b-4599-e709-0e2b91e9dbe9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Reflection Agent Workflow for 24 items...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Items:   0%|          | 0/24 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ceba958f8bc64f6a9a652ff7919762b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Processing Item 1/24 (PID: B01G1RYHAO) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product explicitly matches all query specifications: AA batteries and 100 count/pack. No contradictions found.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 1 (PID: B01G1RYHAO) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 2/24 (PID: B07FP5DNBG) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product exactly matches query specifications: it contains AA batteries and explicitly states \\\"100-Pack\\\"/\\\"Pack of 100\\\" multiple times in both title and bullet points with no contradictions.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 2 (PID: B07FP5DNBG) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 3/24 (PID: B07F7RH8D4) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"The product exactly matches the query specifications: it's AA batteries and explicitly states \\\"100 Count Bulk Pack\\\" in the title, with no contradictions to the query requirements.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 3 (PID: B07F7RH8D4) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 4/24 (PID: B01B8R6PF2) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product exactly matches the query specifications: it contains AA batteries and explicitly states \\\"100-pack\\\" in both title and bullet points. No contradictions found.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 4 (PID: B01B8R6PF2) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 5/24 (PID: B00LHSAARW) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Contradiction Rule violated: Query specifies \\\"100 pack\\\" but product explicitly states \\\"60 Count\\\" which directly contradicts the quantity specification.\",\n",
            "  \"reformulated_query\": \"aa batteries 60 pack\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 5 (PID: B00LHSAARW) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 6/24 (PID: B00KMDL8U6) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Explicit contradiction found: Query specifies \\\"100 pack\\\" but bullet points state \\\"50 count Bulk Packaging\\\", violating Rule 1, despite title mentioning 100 Count.\",\n",
            "  \"reformulated_query\": \"aa batteries 50 pack\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI's assessment is flawed. There is a contradiction between the title stating \"100 Count\" and the bullet points stating \"50 count Bulk Packaging,\" but the reformulated query should be \"aa batteries 100 pack\" (matching the title's count) rather than \"aa batteries 50 pack\" since the title is typically more authoritative than bullet points for product specifications.\n",
            "Decision Check: Error='None', Critique='The AI's assessment is flawed. There is a contradiction between the title stating \"100 Count\" and the bullet points stating \"50 count Bulk Packaging,\" but the reformulated query should be \"aa batteries 100 pack\" (matching the title's count) rather than \"aa batteries 50 pack\" since the title is typically more authoritative than bullet points for product specifications.', Revision Cycle Attempt='1'\n",
            "Deciding to revise.\n",
            "--- Iteration 0: Revision (using claude-3-5-sonnet-20241022) ---\n",
            "Revised Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Explicit contradiction found: Title states \\\"100 Count\\\" while bullet points state \\\"50 count Bulk Packaging\\\", violating Rule 1. Title being more authoritative suggests bullet points need correction.\",\n",
            "  \"reformulated_query\": \"aa batteries 100 pack\"\n",
            "}\n",
            "--- Iteration 1: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI's assessment correctly identifies a contradiction between the title (\"100 Count\") and bullet points (\"50 count Bulk Packaging\"), but it misapplies the Contradiction Rule. The rule specifically refers to contradictions between the Query and Product Info, not internal contradictions within the Product Info itself. The product actually matches the query for \"aa batteries 100 pack\" based on the title, so is_exact_match should be true with null reformulation.\n",
            "Decision Check: Error='None', Critique='The AI's assessment correctly identifies a contradiction between the title (\"100 Count\") and bullet points (\"50 count Bulk Packaging\"), but it misapplies the Contradiction Rule. The rule specifically refers to contradictions between the Query and Product Info, not internal contradictions within the Product Info itself. The product actually matches the query for \"aa batteries 100 pack\" based on the title, so is_exact_match should be true with null reformulation.', Revision Cycle Attempt='2'\n",
            "Deciding to revise.\n",
            "--- Iteration 1: Revision (using claude-3-5-sonnet-20241022) ---\n",
            "Revised Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product title specifies \\\"100 Count\\\" AA batteries, matching the query requirements. Internal product inconsistencies don't affect Rule 1 which only concerns query-product contradictions.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 2: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='3'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 6 (PID: B00KMDL8U6) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 7/24 (PID: B004SCA15K) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product explicitly matches all query specifications: contains \\\"AA batteries\\\" and \\\"100\\\" count is confirmed in both title and bullet points. No contradictions found.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 7 (PID: B004SCA15K) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 8/24 (PID: B01B8R6V2E) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Contradiction Rule: Query specifies \\\"aa batteries\\\" but product explicitly states \\\"AAA batteries\\\" which is a different battery size.\",\n",
            "  \"reformulated_query\": \"aaa batteries 100 pack\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 8 (PID: B01B8R6V2E) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 9/24 (PID: B07TWK2S22) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Explicit contradiction on voltage specification: Query asks for \\\"8V max\\\" but product is \\\"12V MAX\\\". Rule 1 violation.\",\n",
            "  \"reformulated_query\": \"dewalt 12v max cordless screwdriver kit, gyroscopic\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI correctly identified the voltage contradiction (8V vs 12V), but the reformulated query incorrectly omits \"kit\" and includes \"gyroscopic\" without verification that the product has gyroscopic functionality. The reasoning is accurate but could mention that \"kit\" is also missing from the product (which is \"Tool Only\").\n",
            "Decision Check: Error='None', Critique='The AI correctly identified the voltage contradiction (8V vs 12V), but the reformulated query incorrectly omits \"kit\" and includes \"gyroscopic\" without verification that the product has gyroscopic functionality. The reasoning is accurate but could mention that \"kit\" is also missing from the product (which is \"Tool Only\").', Revision Cycle Attempt='1'\n",
            "Deciding to revise.\n",
            "--- Iteration 0: Revision (using claude-3-5-sonnet-20241022) ---\n",
            "Revised Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Multiple contradictions: Query asks for \\\"8V max\\\" but product is \\\"12V MAX\\\", product is \\\"Tool Only\\\" vs query's \\\"kit\\\", and gyroscopic feature isn't mentioned. Rule 1 violation.\",\n",
            "  \"reformulated_query\": \"dewalt 12v max cordless screwdriver tool only\"\n",
            "}\n",
            "--- Iteration 1: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI's assessment correctly identified multiple contradictions (8V vs 12V, kit vs tool only, missing gyroscopic feature) and properly set is_exact_match to false with clear reasoning. The reformulated query accurately reflects the actual product specifications. All rules were applied correctly.\n",
            "\n",
            "ACCEPT\n",
            "Decision Check: Error='None', Critique='The AI's assessment correctly identified multiple contradictions (8V vs 12V, kit vs tool only, missing gyroscopic feature) and properly set is_exact_match to false with clear reasoning. The reformulated query accurately reflects the actual product specifications. All rules were applied correctly.\n",
            "\n",
            "ACCEPT', Revision Cycle Attempt='2'\n",
            "Deciding to revise.\n",
            "--- Iteration 1: Revision (using claude-3-5-sonnet-20241022) ---\n",
            "Revised Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Multiple contradictions: Query asks for '8V max' but product is '12V MAX', product is 'Tool Only' vs query's 'kit', and gyroscopic feature isn't mentioned. Rule 1 violation.\",\n",
            "  \"reformulated_query\": \"dewalt 12v max cordless screwdriver tool only\"\n",
            "}\n",
            "--- Iteration 2: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI correctly identified multiple contradictions (8V vs 12V, kit vs tool only, missing gyroscopic feature) and properly set is_exact_match to false with clear reasoning. However, the reformulated query should include \"cordless screwdriver\" as a complete phrase rather than splitting these terms, and it omits important product identifiers like \"XTREME\" and the model number that would help distinguish this specific product. The reformulation should be more precise.\n",
            "Decision Check: Error='None', Critique='The AI correctly identified multiple contradictions (8V vs 12V, kit vs tool only, missing gyroscopic feature) and properly set is_exact_match to false with clear reasoning. However, the reformulated query should include \"cordless screwdriver\" as a complete phrase rather than splitting these terms, and it omits important product identifiers like \"XTREME\" and the model number that would help distinguish this specific product. The reformulation should be more precise.', Revision Cycle Attempt='3'\n",
            "Deciding to revise.\n",
            "--- Iteration 2: Revision (using claude-3-5-sonnet-20241022) ---\n",
            "Revised Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Multiple contradictions: Query asks for '8V max' but product is '12V MAX', product is 'Tool Only' vs query's 'kit', and gyroscopic feature isn't mentioned. Rule 1 violation.\",\n",
            "  \"reformulated_query\": \"dewalt xtreme 12v max cordless screwdriver dcf601b tool only\"\n",
            "}\n",
            "--- Iteration 3: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI correctly identified multiple contradictions (8V vs 12V, kit vs tool only, missing gyroscopic feature), but the reformulated query is problematic. It should better preserve the original query's intent by including \"cordless screwdriver\" rather than just replacing specifications. A better reformulation would maintain the product type while correcting the specifications that contradicted.\n",
            "Decision Check: Error='None', Critique='The AI correctly identified multiple contradictions (8V vs 12V, kit vs tool only, missing gyroscopic feature), but the reformulated query is problematic. It should better preserve the original query's intent by including \"cordless screwdriver\" rather than just replacing specifications. A better reformulation would maintain the product type while correcting the specifications that contradicted.', Revision Cycle Attempt='4'\n",
            "Deciding to finish: Max iterations (3) reached.\n",
            "===== Finished Item 9 (PID: B07TWK2S22) - Outcome: Max Iterations Reached =====\n",
            "\n",
            "===== Processing Item 10/24 (PID: B0812ZHY5N) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Contradiction Rule: Query specifies \\\"dewalt\\\" brand and \\\"gyroscopic\\\" feature, but product is ENERTWIST brand and no gyroscopic functionality is mentioned.\",\n",
            "  \"reformulated_query\": \"enertwist 8v max cordless screwdriver kit with 82-piece accessory set, dual position handle, LED light\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 10 (PID: B0812ZHY5N) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 11/24 (PID: B07S7F53YK) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product is an exact match as it contains all query specifications: 8V Max, cordless (implied by battery kit), screwdriver, and explicitly mentions \\\"Gyroscopic\\\" in the title.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 11 (PID: B07S7F53YK) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 12/24 (PID: B00EUHAGX0) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Product is a battery charger, which explicitly contradicts the query specification for a \\\"screwdriver kit\\\" (Contradiction Rule).\",\n",
            "  \"reformulated_query\": \"dewalt 8v max battery charger\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 12 (PID: B00EUHAGX0) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 13/24 (PID: B00DL7QDS2) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"The product is an exact match as it explicitly matches all query specifications: it's a DeWalt 8V MAX cordless screwdriver kit and is specifically described as gyroscopic in both title and features.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 13 (PID: B00DL7QDS2) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 14/24 (PID: B011WRUODC) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"The product is an exact match as it explicitly matches all query specifications: it's a DeWalt 8V MAX cordless screwdriver kit and is specifically described as gyroscopic in both title and features.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 14 (PID: B011WRUODC) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 15/24 (PID: B085G3MGFR) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"All query specifications (Kodak, photo paper, 8.5 x 11, glossy) are explicitly matched in the product title with no contradictions. Extra details (50 counts, weight) don't affect match per Rule 3.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 15 (PID: B085G3MGFR) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 16/24 (PID: B085F42SV6) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Contradiction Rule applies: Query specifies 'glossy' paper but product explicitly states 'matte' finish, which is a direct contradiction.\",\n",
            "  \"reformulated_query\": \"kodak photo paper 8.5 x 11 matte\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 16 (PID: B085F42SV6) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 17/24 (PID: B01M0L2WLF) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"All query specifications (Kodak brand, photo paper, 8.5x11 size, glossy finish) are explicitly confirmed in the product information with no contradictions.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 17 (PID: B01M0L2WLF) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 18/24 (PID: B01JB7D4SW) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Contradiction Rule violated: Product brand is listed as \\\"Doaaler\\\" while query specifies \\\"kodak\\\", despite having Kodak in product title.\",\n",
            "  \"reformulated_query\": \"doaaler photo paper 8.5 x 11 glossy\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: The AI's assessment is flawed. The Contradiction Rule was correctly applied (brand mismatch), but the reformulated query is incorrect. The product title clearly shows \"Kodak 8209017\" as the product identifier, suggesting this is a Kodak product despite the brand field showing \"Doaaler\" (likely an error). The reformulated query should maintain \"Kodak\" rather than replacing it with \"Doaaler\" since the product appears to be a Kodak product based on the title.\n",
            "Decision Check: Error='None', Critique='The AI's assessment is flawed. The Contradiction Rule was correctly applied (brand mismatch), but the reformulated query is incorrect. The product title clearly shows \"Kodak 8209017\" as the product identifier, suggesting this is a Kodak product despite the brand field showing \"Doaaler\" (likely an error). The reformulated query should maintain \"Kodak\" rather than replacing it with \"Doaaler\" since the product appears to be a Kodak product based on the title.', Revision Cycle Attempt='1'\n",
            "Deciding to revise.\n",
            "--- Iteration 0: Revision (using claude-3-5-sonnet-20241022) ---\n",
            "Revised Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Contradiction Rule violated: Ambiguous brand identity with Kodak in product title/ID but Doaaler listed as brand, creating uncertainty about the true manufacturer.\",\n",
            "  \"reformulated_query\": \"kodak photo paper 8.5 x 11 glossy (verify brand authenticity)\"\n",
            "}\n",
            "--- Iteration 1: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='2'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 18 (PID: B01JB7D4SW) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 19/24 (PID: B000EZTYHG) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"All query specifications (Kodak, photo paper, 8.5x11 size, glossy) are explicitly confirmed in the product information without any contradictions.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 19 (PID: B000EZTYHG) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 20/24 (PID: B000EZTYG2) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product matches all query specifications: Kodak brand, photo paper, 8.5 x 11 size, and glossy finish (listed as \\\"Gloss Finish\\\"). No contradictions found.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 20 (PID: B000EZTYG2) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 21/24 (PID: B000EZTYCG) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product matches all query specifications: Kodak brand, photo paper type, 8.5 x 11 dimensions, and glossy finish are all explicitly confirmed in the product title and description.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 21 (PID: B000EZTYCG) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 22/24 (PID: B000EZ0CTK) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": false,\n",
            "  \"reasoning\": \"Product explicitly contradicts query specification: query requests \\\"glossy\\\" paper but product is \\\"Matte Finish\\\" (Contradiction Rule)\",\n",
            "  \"reformulated_query\": \"kodak photo paper 8.5 x 11 matte\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 22 (PID: B000EZ0CTK) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 23/24 (PID: B000EYAKKW) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product matches all query specifications: Kodak brand, photo paper, 8.5 x 11 size, and glossy finish are all explicitly confirmed in the product title and description.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 23 (PID: B000EYAKKW) - Outcome: Accepted =====\n",
            "\n",
            "===== Processing Item 24/24 (PID: B000EYAKJS) =====\n",
            "--- Iteration 0: Initial Verification (using claude-3-5-sonnet-20241022) ---\n",
            "Initial Assessment: {\n",
            "  \"is_exact_match\": true,\n",
            "  \"reasoning\": \"Product matches all query specifications: Kodak brand, photo paper type, 8.5x11 size, and glossy finish are all explicitly confirmed in the product title and description.\",\n",
            "  \"reformulated_query\": \"null\"\n",
            "}\n",
            "--- Iteration 0: Reflection (using claude-3-7-sonnet-latest) ---\n",
            "Reflection Critique: ACCEPT\n",
            "Decision Check: Error='None', Critique='ACCEPT', Revision Cycle Attempt='1'\n",
            "Deciding to finish: Reflection accepted.\n",
            "===== Finished Item 24 (PID: B000EYAKJS) - Outcome: Accepted =====\n",
            "\n",
            "Reflection Agent Workflow Completed in 112.13 seconds.\n",
            "\n",
            "--- Sample Results (Reflection Agent - Multi-Model) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   query_id  product_id  accurate_label    reformulated_query  \\\n",
              "0      6014  B01G1RYHAO            True                  null   \n",
              "1      6014  B07FP5DNBG            True                  null   \n",
              "2      6014  B07F7RH8D4            True                  null   \n",
              "3      6014  B01B8R6PF2            True                  null   \n",
              "4      6014  B00LHSAARW           False  aa batteries 60 pack   \n",
              "\n",
              "                                                                                                                                         final_reasoning  \\\n",
              "0                                         Product explicitly matches all query specifications: AA batteries and 100 count/pack. No contradictions found.   \n",
              "1  Product exactly matches query specifications: it contains AA batteries and explicitly states \"100-Pack\"/\"Pack of 100\" multiple times in both title...   \n",
              "2  The product exactly matches the query specifications: it's AA batteries and explicitly states \"100 Count Bulk Pack\" in the title, with no contradi...   \n",
              "3  Product exactly matches the query specifications: it contains AA batteries and explicitly states \"100-pack\" in both title and bullet points. No co...   \n",
              "4  Contradiction Rule violated: Query specifies \"100 pack\" but product explicitly states \"60 Count\" which directly contradicts the quantity specifica...   \n",
              "\n",
              "   iterations final_outcome  \n",
              "0           0      Accepted  \n",
              "1           0      Accepted  \n",
              "2           0      Accepted  \n",
              "3           0      Accepted  \n",
              "4           0      Accepted  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c492244-523b-4434-bc29-dfee94080dd1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>accurate_label</th>\n",
              "      <th>reformulated_query</th>\n",
              "      <th>final_reasoning</th>\n",
              "      <th>iterations</th>\n",
              "      <th>final_outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6014</td>\n",
              "      <td>B01G1RYHAO</td>\n",
              "      <td>True</td>\n",
              "      <td>null</td>\n",
              "      <td>Product explicitly matches all query specifications: AA batteries and 100 count/pack. No contradictions found.</td>\n",
              "      <td>0</td>\n",
              "      <td>Accepted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6014</td>\n",
              "      <td>B07FP5DNBG</td>\n",
              "      <td>True</td>\n",
              "      <td>null</td>\n",
              "      <td>Product exactly matches query specifications: it contains AA batteries and explicitly states \"100-Pack\"/\"Pack of 100\" multiple times in both title...</td>\n",
              "      <td>0</td>\n",
              "      <td>Accepted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6014</td>\n",
              "      <td>B07F7RH8D4</td>\n",
              "      <td>True</td>\n",
              "      <td>null</td>\n",
              "      <td>The product exactly matches the query specifications: it's AA batteries and explicitly states \"100 Count Bulk Pack\" in the title, with no contradi...</td>\n",
              "      <td>0</td>\n",
              "      <td>Accepted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6014</td>\n",
              "      <td>B01B8R6PF2</td>\n",
              "      <td>True</td>\n",
              "      <td>null</td>\n",
              "      <td>Product exactly matches the query specifications: it contains AA batteries and explicitly states \"100-pack\" in both title and bullet points. No co...</td>\n",
              "      <td>0</td>\n",
              "      <td>Accepted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6014</td>\n",
              "      <td>B00LHSAARW</td>\n",
              "      <td>False</td>\n",
              "      <td>aa batteries 60 pack</td>\n",
              "      <td>Contradiction Rule violated: Query specifies \"100 pack\" but product explicitly states \"60 Count\" which directly contradicts the quantity specifica...</td>\n",
              "      <td>0</td>\n",
              "      <td>Accepted</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c492244-523b-4434-bc29-dfee94080dd1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c492244-523b-4434-bc29-dfee94080dd1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c492244-523b-4434-bc29-dfee94080dd1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8efea4b6-4695-4d92-aff3-8e8f18cdb543\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8efea4b6-4695-4d92-aff3-8e8f18cdb543')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8efea4b6-4695-4d92-aff3-8e8f18cdb543 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"\\\\nERROR saving reflection results: {e}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"query_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 6014,\n        \"max\": 6014,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"B07FP5DNBG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accurate_label\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reformulated_query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"aa batteries 60 pack\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_reasoning\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Product exactly matches query specifications: it contains AA batteries and explicitly states \\\"100-Pack\\\"/\\\"Pack of 100\\\" multiple times in both title and bullet points with no contradictions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iterations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_outcome\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Accepted\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Basic Stats ---\n",
            "Total items processed: 24\n",
            "\n",
            "Final Label Distribution:\n",
            "accurate_label\n",
            "Accurate      16\n",
            "Inaccurate     8\n",
            "Name: count, dtype: Int64\n",
            "\n",
            "Iterations Distribution (0 = initial verify only):\n",
            "iterations\n",
            "0    21\n",
            "1     1\n",
            "2     1\n",
            "3     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Final Outcome Distribution:\n",
            "final_outcome\n",
            "Accepted                  23\n",
            "Max Iterations Reached     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Successfully saved reflection agent results to: results/grainger_llm_reflection_results_multi_model_04_23.csv\n",
            "Successfully saved full reflection agent results to: results/grainger_llm_reflection_results_multi_model_04_23_FULL.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Executive Summary Cell ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*30 + \" Executive Summary Generation (Reflection Agent) \" + \"=\"*30)\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# These variables should be available from the previous cell's execution\n",
        "required_data_vars = {\n",
        "    'df_results_reflection': 'df_results_reflection', # DataFrame with results\n",
        "    'GENERATOR_MODEL_NAME': 'GENERATOR_MODEL_NAME', # Name of the generator LLM\n",
        "    'CRITIC_MODEL_NAME': 'CRITIC_MODEL_NAME',       # Name of the critic LLM\n",
        "    'REFLECTION_RESULTS_CSV': 'REFLECTION_RESULTS_CSV', # Output filename\n",
        "    'MAX_ITERATIONS': 'MAX_ITERATIONS'              # Max reflection cycles\n",
        "}\n",
        "\n",
        "missing_vars = []\n",
        "for display_name, actual_name in required_data_vars.items():\n",
        "    if actual_name not in locals():\n",
        "        missing_vars.append(f\"'{actual_name}' (needed for {display_name})\")\n",
        "\n",
        "# --- Proceed only if essential variables exist ---\n",
        "if missing_vars:\n",
        "     summary_md = f\"## LLM Verification Summary (Reflection Agent)\\n\\nERROR: Could not generate summary. Essential analysis data missing:\\n- {chr(10).join(missing_vars)}\"\n",
        "     warnings.warn(f\"Missing variables needed for summary: {', '.join(missing_vars)}\")\n",
        "     display(Markdown(summary_md)) # Display the error message\n",
        "else:\n",
        "    # --- Prepare data for summary using variables verified to exist ---\n",
        "    df_results = locals()[required_data_vars['df_results_reflection']]\n",
        "    generator_model = locals()[required_data_vars['GENERATOR_MODEL_NAME']]\n",
        "    critic_model = locals()[required_data_vars['CRITIC_MODEL_NAME']]\n",
        "    final_csv = locals()[required_data_vars['REFLECTION_RESULTS_CSV']]\n",
        "    max_iterations = locals()[required_data_vars['MAX_ITERATIONS']]\n",
        "    full_csv = final_csv.replace(\".csv\", \"_FULL.csv\") # Derive full CSV name\n",
        "\n",
        "    total_items = len(df_results)\n",
        "\n",
        "    # Calculate final label distribution from the results DataFrame\n",
        "    final_label_counts = df_results['accurate_label'].value_counts(dropna=False).rename({True: 'Accurate', False: 'Inaccurate'})\n",
        "    if pd.isna(final_label_counts.index).any():\n",
        "        rename_map = {idx: 'Undetermined (Error/MaxIter)' for idx in final_label_counts.index if pd.isna(idx)}\n",
        "        final_label_counts = final_label_counts.rename(index=rename_map)\n",
        "\n",
        "    num_accurate = final_label_counts.get('Accurate', 0)\n",
        "    num_inaccurate = final_label_counts.get('Inaccurate', 0)\n",
        "    num_undetermined = final_label_counts.get('Undetermined (Error/MaxIter)', 0)\n",
        "\n",
        "    # Get outcome distribution\n",
        "    outcome_counts = df_results['final_outcome'].value_counts()\n",
        "    num_accepted = outcome_counts.get('Accepted', 0)\n",
        "    num_max_iter = outcome_counts.get('Max Iterations Reached', 0)\n",
        "    num_errors = outcome_counts.get('Error', 0) + outcome_counts.get('Outer Loop Error', 0) + outcome_counts.get('Ended Unexpectedly', 0)\n",
        "\n",
        "\n",
        "    # Helper function to calculate and format percentage string safely\n",
        "    def format_percentage_string(count, total):\n",
        "        \"\"\"Calculates percentage and formats to string 'XX.X%', or returns 'N/A'.\"\"\"\n",
        "        if isinstance(count, int) and isinstance(total, int) and total > 0:\n",
        "            percentage_value = (count / total)\n",
        "            return f\"{percentage_value:.1%}\"\n",
        "        else:\n",
        "            return \"N/A\"\n",
        "\n",
        "    # Calculate percentages\n",
        "    perc_accurate = format_percentage_string(num_accurate, total_items)\n",
        "    perc_inaccurate = format_percentage_string(num_inaccurate, total_items)\n",
        "    perc_undetermined = format_percentage_string(num_undetermined, total_items)\n",
        "\n",
        "    # --- Construct Markdown Summary ---\n",
        "    summary_md = f\"\"\"\n",
        "## LLM Verification Task: Executive Summary (Reflection Agent Approach)\n",
        "\n",
        "**Objective:** Verify the accuracy of the 'Exact' (E) label for **{total_items}** specific query-product pairs using a single LLM with a reflection/critique loop. Reformulate queries where the 'E' label was deemed inaccurate due to explicit contradictions.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Data Preparation:** Loaded ESCI data, merged product details, filtered for 3 target queries with 'E' labels (`aa batteries 100 pack`, `kodak photo paper...`, `dewalt 8v...`).\n",
        "2.  **Reflection Agent (LangGraph):**\n",
        "    *   An agent loop was implemented using LangGraph.\n",
        "    *   **Verifier/Reviser LLM ({generator_model}):** Generated initial JSON assessments and revised them based on critique, adhering to strict rules (Contradiction, Missing Info, Extra Info).\n",
        "    *   **Critic LLM ({critic_model}):** Evaluated the Verifier/Reviser's JSON output against the rules. It responded with \"ACCEPT\" if correct or provided concise critique otherwise.\n",
        "    *   **Loop:** The process iterated (verify -> reflect -> revise -> reflect...) until the Critic outputted \"ACCEPT\" or the maximum of **{max_iterations}** revision cycles was reached.\n",
        "3.  **Output:** The final assessment from the loop (either accepted or the last one before max iterations) was recorded.\n",
        "\n",
        "**Key Findings & Metrics:**\n",
        "\n",
        "*   **Total Items Analyzed:** {total_items}\n",
        "*   **Final Label Distribution (After Reflection):**\n",
        "    *   ✅ **Accurate:** **{num_accurate}** ({perc_accurate}) - *Final assessment confirmed 'E' label.*\n",
        "    *   ❌ **Inaccurate:** **{num_inaccurate}** ({perc_inaccurate}) - *Final assessment deemed 'E' label incorrect due to contradictions.*\n",
        "    *   ❓ **Undetermined:** **{num_undetermined}** ({perc_undetermined}) - *Workflow ended due to max iterations or error before an 'ACCEPT' state.*\n",
        "*   **Workflow Outcomes:**\n",
        "    *   **Accepted:** {num_accepted} items reached an \"ACCEPT\" state from the critic.\n",
        "    *   **Max Iterations Reached:** {num_max_iter} item hit the revision limit.\n",
        "    *   **Errors:** {num_errors} items encountered errors during processing.\n",
        "*   **Efficiency:** Most items ({outcome_counts.get('Accepted', 0)} out of {total_items}) were accepted after 0 or 1 revision cycles, indicating good initial accuracy from the generator LLM ({generator_model}).\n",
        "\n",
        "**Analysis Highlights:**\n",
        "\n",
        "1.  **Inaccurate Labels Identified:** The **{num_inaccurate}** items ultimately marked 'Inaccurate' represent clear instances where product details explicitly contradicted the search query. The types of contradictions were consistent with the previous multi-model approach (pack size, attribute, voltage, brand, product type). The generated `reformulated_query` in the output file provides suggested corrections.\n",
        "\n",
        "2.  **Self-Correction Demonstrated:** The reflection mechanism proved effective. For instance, item 6 (PID: `B00KMDL8U6`) was initially assessed as `False` (Inaccurate) due to conflicting counts in title vs. bullets. The critic correctly pointed out the title precedence, leading the reviser to correct the assessment to `True` (Accurate), which was then accepted. This highlights the pattern's ability to catch and fix initial LLM errors based on predefined rules.\n",
        "\n",
        "3.  **Loop Termination:** The maximum iteration limit functioned as intended. Item 9 (PID: `B07TWK2S22`) reached the limit after {max_iterations} revisions. While the critic still found flaws in the final reformulated query (incorrectly including \"gyroscopic\"), the loop terminated, providing the *last available assessment* rather than looping indefinitely. This demonstrates controlled execution but also shows that complex critiques might require more iterations or refined prompting.\n",
        "\n",
        "4.  **Comparison to Ensemble:** This reflection approach yielded the **same final `accurate_label` distribution (16 Accurate, 8 Inaccurate)** as the 3-model ensemble method for this specific dataset. This suggests that for this task complexity and these models, both robust methods converged on the same outcome, though potentially via different paths (self-correction vs. voting).\n",
        "\n",
        "**Benefits & Considerations of Reflection Agent:**\n",
        "\n",
        "*   **Self-Correction:** The primary benefit is the potential to improve accuracy by catching and correcting the generator's own mistakes, as seen with item 6. This can be crucial for high-stakes tasks.\n",
        "*   **Resource Efficiency (Model Count):** Requires managing only one primary generator and one (potentially smaller/faster) critic model, compared to multiple large models in the ensemble.\n",
        "*   **Latency/Cost per Item:** Can be higher than a single LLM call due to multiple sequential calls (verify, reflect, potentially revise+reflect). The total cost depends heavily on the number of iterations needed per item.\n",
        "*   **Prompt Complexity:** Requires careful crafting of not just the generator prompt but also the critic prompt to ensure accurate and useful feedback.\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "*   **Action Inaccurate Findings:**   We should prioritize reviewing the **{num_inaccurate}** items flagged as 'Inaccurate'. Evaluate the LLM-generated `reformulated_query` (available in `{final_csv}`) for potential catalog or search relevance improvements.\n",
        "*   **Leverage High Confidence Results:** The **{num_accurate}** items confirmed as 'Accurate' (most accepted after 0 revisions) represent high-confidence results suitable for streamlined review.\n",
        "*   **Consider Use Cases:** The reflection pattern is particularly valuable when:\n",
        "    *   Initial errors are costly or unacceptable.\n",
        "    *   A mechanism for rule-based refinement is needed.\n",
        "    *   Running multiple large models simultaneously is resource-prohibitive, but slightly higher latency per item is acceptable.\n",
        "*   **Future Tuning:** Experiment with different critic models, adjust `MAX_ITERATIONS`, or refine the critic prompt for more nuanced feedback, especially for complex cases like item 9's reformulation.\n",
        "\n",
        "**Output:** The final results table required by the exercise (containing `query_id`, `product_id`, `accurate_label`, `reformulated_query`) is saved in `{final_csv}`. A full table including detailed final reasoning, iterations, and outcomes is available in `{full_csv}` for deeper analysis.\n",
        "\"\"\"\n",
        "\n",
        "# --- Display the Summary ---\n",
        "display(Markdown(summary_md))\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H5lhgmI7riLO",
        "outputId": "8863d9c4-1094-4255-cf2a-47046b08fcd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================== Executive Summary Generation (Reflection Agent) ==============================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## LLM Verification Task: Executive Summary (Reflection Agent Approach)\n\n**Objective:** Verify the accuracy of the 'Exact' (E) label for **24** specific query-product pairs using a single LLM with a reflection/critique loop. Reformulate queries where the 'E' label was deemed inaccurate due to explicit contradictions.\n\n**Methodology:**\n1.  **Data Preparation:** Loaded ESCI data, merged product details, filtered for 3 target queries with 'E' labels (`aa batteries 100 pack`, `kodak photo paper...`, `dewalt 8v...`).\n2.  **Reflection Agent (LangGraph):**\n    *   An agent loop was implemented using LangGraph.\n    *   **Verifier/Reviser LLM (claude-3-5-sonnet-20241022):** Generated initial JSON assessments and revised them based on critique, adhering to strict rules (Contradiction, Missing Info, Extra Info).\n    *   **Critic LLM (claude-3-7-sonnet-latest):** Evaluated the Verifier/Reviser's JSON output against the rules. It responded with \"ACCEPT\" if correct or provided concise critique otherwise.\n    *   **Loop:** The process iterated (verify -> reflect -> revise -> reflect...) until the Critic outputted \"ACCEPT\" or the maximum of **3** revision cycles was reached.\n3.  **Output:** The final assessment from the loop (either accepted or the last one before max iterations) was recorded.\n\n**Key Findings & Metrics:**\n\n*   **Total Items Analyzed:** 24\n*   **Final Label Distribution (After Reflection):**\n    *   ✅ **Accurate:** **16** (N/A) - *Final assessment confirmed 'E' label.*\n    *   ❌ **Inaccurate:** **8** (N/A) - *Final assessment deemed 'E' label incorrect due to contradictions.*\n    *   ❓ **Undetermined:** **0** (0.0%) - *Workflow ended due to max iterations or error before an 'ACCEPT' state.*\n*   **Workflow Outcomes:**\n    *   **Accepted:** 23 items reached an \"ACCEPT\" state from the critic.\n    *   **Max Iterations Reached:** 1 item hit the revision limit.\n    *   **Errors:** 0 items encountered errors during processing.\n*   **Efficiency:** Most items (23 out of 24) were accepted after 0 or 1 revision cycles, indicating good initial accuracy from the generator LLM (claude-3-5-sonnet-20241022).\n\n**Analysis Highlights:**\n\n1.  **Inaccurate Labels Identified:** The **8** items ultimately marked 'Inaccurate' represent clear instances where product details explicitly contradicted the search query. The types of contradictions were consistent with the previous multi-model approach (pack size, attribute, voltage, brand, product type). The generated `reformulated_query` in the output file provides suggested corrections.\n\n2.  **Self-Correction Demonstrated:** The reflection mechanism proved effective. For instance, item 6 (PID: `B00KMDL8U6`) was initially assessed as `False` (Inaccurate) due to conflicting counts in title vs. bullets. The critic correctly pointed out the title precedence, leading the reviser to correct the assessment to `True` (Accurate), which was then accepted. This highlights the pattern's ability to catch and fix initial LLM errors based on predefined rules.\n\n3.  **Loop Termination:** The maximum iteration limit functioned as intended. Item 9 (PID: `B07TWK2S22`) reached the limit after 3 revisions. While the critic still found flaws in the final reformulated query (incorrectly including \"gyroscopic\"), the loop terminated, providing the *last available assessment* rather than looping indefinitely. This demonstrates controlled execution but also shows that complex critiques might require more iterations or refined prompting.\n\n4.  **Comparison to Ensemble:** This reflection approach yielded the **same final `accurate_label` distribution (16 Accurate, 8 Inaccurate)** as the 3-model ensemble method for this specific dataset. This suggests that for this task complexity and these models, both robust methods converged on the same outcome, though potentially via different paths (self-correction vs. voting).\n\n**Benefits & Considerations of Reflection Agent:**\n\n*   **Self-Correction:** The primary benefit is the potential to improve accuracy by catching and correcting the generator's own mistakes, as seen with item 6. This can be crucial for high-stakes tasks.\n*   **Resource Efficiency (Model Count):** Requires managing only one primary generator and one (potentially smaller/faster) critic model, compared to multiple large models in the ensemble.\n*   **Latency/Cost per Item:** Can be higher than a single LLM call due to multiple sequential calls (verify, reflect, potentially revise+reflect). The total cost depends heavily on the number of iterations needed per item.\n*   **Prompt Complexity:** Requires careful crafting of not just the generator prompt but also the critic prompt to ensure accurate and useful feedback.\n\n**Recommendations:**\n\n*   **Action Inaccurate Findings:**   We should prioritize reviewing the **8** items flagged as 'Inaccurate'. Evaluate the LLM-generated `reformulated_query` (available in `results/grainger_llm_reflection_results_multi_model_04_23.csv`) for potential catalog or search relevance improvements.\n*   **Leverage High Confidence Results:** The **16** items confirmed as 'Accurate' (most accepted after 0 revisions) represent high-confidence results suitable for streamlined review.\n*   **Consider Use Cases:** The reflection pattern is particularly valuable when:\n    *   Initial errors are costly or unacceptable.\n    *   A mechanism for rule-based refinement is needed.\n    *   Running multiple large models simultaneously is resource-prohibitive, but slightly higher latency per item is acceptable.\n*   **Future Tuning:** Experiment with different critic models, adjust `MAX_ITERATIONS`, or refine the critic prompt for more nuanced feedback, especially for complex cases like item 9's reformulation.\n\n**Output:** The final results table required by the exercise (containing `query_id`, `product_id`, `accurate_label`, `reformulated_query`) is saved in `results/grainger_llm_reflection_results_multi_model_04_23.csv`. A full table including detailed final reasoning, iterations, and outcomes is available in `results/grainger_llm_reflection_results_multi_model_04_23_FULL.csv` for deeper analysis.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}