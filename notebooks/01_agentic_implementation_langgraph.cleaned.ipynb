{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9zGygGhgXss"
   },
   "outputs": [],
   "source": [
    "!pip install langgraph langchain python-dotenv langchain-anthropic pandas pyarrow pydantic -q -U\n",
    "!apt-get install -qq graphviz libgraphviz-dev # Install system library quietly\n",
    "!pip install -q pygraphviz # Install Python wrapper quietly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DJjH8WkJnTYN",
    "outputId": "28988496-dc8e-48bd-c457-b141f5cc161e"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "01_agentic_implementation_langgraph.ipynb\n",
    "\n",
    "Alternative solution using a LangGraph Reflection Agent pattern\n",
    "for the Grainger LLM Verification task, using separate Generator and Critic models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuwM1jk_gzY5",
    "outputId": "2459cb2f-338b-4084-9cbe-9cbe1ee92425"
   },
   "outputs": [],
   "source": [
    "# --- Environment Setup ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np # Import numpy for NA checks if needed\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import warnings\n",
    "from typing import TypedDict, List, Optional, Annotated, Dict, Any\n",
    "from IPython.display import Image, display\n",
    "import logging\n",
    "\n",
    "# LangChain & LangGraph imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # Using v1 for broader compatibility\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- Load Environment Variables ---\n",
    "## --- Load Environment Variables using Colab Secrets ---\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"ANTHROPIC_API_KEY not found in Colab Secrets. Please add it.\")\n",
    "    else:\n",
    "        print(\"ANTHROPIC_API_KEY loaded successfully from Colab Secrets.\")\n",
    "    # No need to set os.environ explicitly if passing the key directly to the client later\n",
    "except ImportError:\n",
    "    print(\"Could not import google.colab.userdata. Are you running in Colab?\")\n",
    "    # Fallback or error handling if not in Colab\n",
    "    raise EnvironmentError(\"Not running in Colab and Colab Secrets unavailable.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# LLM Initialization\n",
    "GENERATOR_MODEL_NAME = \"claude-3-5-sonnet-20241022\" #\n",
    "CRITIC_MODEL_NAME = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "try:\n",
    "    # Pass the key explicitly for clarity and robustness\n",
    "    generator_llm = ChatAnthropic(\n",
    "        model=GENERATOR_MODEL_NAME,\n",
    "        temperature=0.1,\n",
    "        anthropic_api_key=ANTHROPIC_API_KEY # Pass the loaded key\n",
    "    )\n",
    "    print(f\"Generator LLM initialized: {GENERATOR_MODEL_NAME}\")\n",
    "\n",
    "    critic_llm = ChatAnthropic(\n",
    "        model=CRITIC_MODEL_NAME,\n",
    "        temperature=0.0,\n",
    "        anthropic_api_key=ANTHROPIC_API_KEY # Pass the loaded key\n",
    "    )\n",
    "    print(f\"Critic LLM initialized: {CRITIC_MODEL_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Anthropic models: {e}\")\n",
    "    raise\n",
    "\n",
    "# .configuration: Data Path, MAX_ITERATIONS, Output file\n",
    "PROCESSED_DATA_PATH = '/content/drive/MyDrive/Colab_Data/Grainger_LLM_Exercise/filtered_data_with_context_2025-04-22.parquet'\n",
    "MAX_ITERATIONS = 3\n",
    "REFLECTION_RESULTS_CSV = \"results/grainger_llm_reflection_results_multi_model_04_23.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TcxxB1noItX",
    "outputId": "eefa7cc3-bb1c-4556-bbce-897a5f9f1133"
   },
   "outputs": [],
   "source": [
    "# --- Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "try:\n",
    "    # Add force_remount=True\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "    raise SystemExit(\"Google Drive mount failed. Cannot proceed.\")\n",
    "\n",
    "# Optional: Verify the file exists after mounting\n",
    "import os\n",
    "# Use the same PROCESSED_DATA_PATH variable\n",
    "if os.path.exists(PROCESSED_DATA_PATH):\n",
    "    print(f\"Verified: File exists at {PROCESSED_DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"WARNING: File NOT found at {PROCESSED_DATA_PATH} after mounting.\")\n",
    "    print(\"Please double-check the path and filename in your Google Drive.\")\n",
    "    # List contents of the expected directory to help debug\n",
    "    expected_dir = os.path.dirname(PROCESSED_DATA_PATH)\n",
    "    if os.path.exists(expected_dir):\n",
    "         print(f\"Contents of {expected_dir}:\")\n",
    "         try:\n",
    "             print(os.listdir(expected_dir))\n",
    "         except Exception as list_e:\n",
    "             print(f\"Could not list directory contents: {list_e}\")\n",
    "    else:\n",
    "         print(f\"Directory {expected_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wocK6lhQmG6Y",
    "outputId": "1066e81d-1aff-4cda-9572-8095472629b5"
   },
   "outputs": [],
   "source": [
    "# --- Load Preprocessed Data from end to end worfklow notebook  step 6.6---\n",
    "try:\n",
    "    # Ensure the path is correct for our environment (Colab Drive vs local) - saved data in colab\n",
    "    df_input = pd.read_parquet(PROCESSED_DATA_PATH)\n",
    "    print(f\"Loaded data from {PROCESSED_DATA_PATH}. Shape: {df_input.shape}\")\n",
    "    # Ensure required columns are present\n",
    "    required_cols = ['query_id', 'product_id', 'query', 'llm_product_context']\n",
    "    if not all(col in df_input.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df_input.columns]\n",
    "        raise ValueError(f\"Missing required columns in input data: {missing}\")\n",
    "    if df_input.empty:\n",
    "         raise ValueError(\"Input DataFrame is empty.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Processed data file not found at {PROCESSED_DATA_PATH}\")\n",
    "    print(\"Please ensure the file exists or run the data preparation steps first.\")\n",
    "    # Handle error appropriately, e.g., raise SystemExit or run data prep\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "# --- Pydantic Model for Structured Output ---\n",
    "class VerificationResult(BaseModel):\n",
    "    \"\"\"Structured output for the verification task.\"\"\"\n",
    "    is_exact_match: bool = Field(description=\"True if the product is an exact match based on the rules, False otherwise.\")\n",
    "    reasoning: str = Field(description=\"Concise explanation citing the specific rule and evidence (contradiction or confirmation/missing info). Max 1-2 sentences.\")\n",
    "    reformulated_query: Optional[str] = Field(description=\"If is_exact_match is False, the corrected query. Must be null if is_exact_match is True.\")\n",
    "\n",
    "# --- Prompts ---\n",
    "VERIFIER_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an extremely precise AI Data Quality Analyst. Your task is to verify if the Product Information makes the product an \"Exact\" match for the Search Query based *only* on the provided text and strict rules.\n",
    "\n",
    "**Definition of Exact Match ('E'):**\n",
    "The Product is relevant for the Query AND satisfies ALL specifications mentioned in the Query.\n",
    "\n",
    "**Crucial Rules:**\n",
    "1.  **Contradiction Rule:** If Product Info EXPLICITLY CONTRADICTS a Query specification (e.g., query \"100 pack\", product \"50 count\"; query \"without shams\", product \"includes shams\"), it's NOT Exact. Mark `is_exact_match: false`.\n",
    "2.  **Missing Information Rule:** If Product Info DOES NOT MENTION a query spec (e.g., query \"gyroscopic\", feature not mentioned), assume it MIGHT satisfy it. DO NOT mark as non-Exact based *only* on missing info. It remains an Exact candidate (`is_exact_match: true` unless Rule 1 applies).\n",
    "3.  **Extra Information Rule:** Extra product details NOT in the query are acceptable if Rule 1 isn't violated.\n",
    "\n",
    "**Input:**\n",
    "Search Query: \"{query}\"\n",
    "Product Information:\n",
    "--- START ---\n",
    "{product_context}\n",
    "--- END ---\n",
    "\n",
    "**Output:**\n",
    "Respond ONLY with a valid JSON object adhering to the VerificationResult schema. Provide clear reasoning based *directly* on the rules and text. If false, formulate a concise, accurate query based on the product details provided. If true, reformulated_query MUST be null.\"\"\"),\n",
    "    (\"human\", \"Analyze the provided query and product information according to the rules and provide your assessment in the required JSON format.\")\n",
    "])\n",
    "\n",
    "REFLECTOR_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a meticulous Quality Assurance expert evaluating an AI's verification judgment. Your goal is to ensure the AI strictly followed the rules and produced a correct, well-reasoned assessment.\n",
    "\n",
    "**Evaluation Context:**\n",
    "Original Search Query: \"{query}\"\n",
    "Product Information:\n",
    "--- START ---\n",
    "{product_context}\n",
    "--- END ---\n",
    "\n",
    "AI Verifier's Assessment (JSON):\n",
    "```json\n",
    "{assessment_json}\n",
    "\n",
    "Your Task:\n",
    "Critically evaluate the AI Verifier's Assessment based only on the provided Query, Product Information, and these Crucial Rules:\n",
    "Contradiction Rule: Was any EXPLICIT CONTRADICTION between Query and Product Info correctly identified (leading to is_exact_match: false)? Or was a contradiction missed?\n",
    "Missing Information Rule: Was the rule applied correctly? Did the AI correctly determine is_exact_match: true when information was merely missing (and no contradiction existed)? Or did it incorrectly flag missing info as a reason for is_exact_match: false?\n",
    "Reasoning Quality: Is the reasoning accurate, concise, and directly supported by the text and rules? Does it clearly state the basis (contradiction found, or spec met/missing)?\n",
    "Reformulation Quality: If is_exact_match: false, is the reformulated_query accurate based on the product info, relevant to the original query's intent, and concise? Is it correctly null if is_exact_match: true?\n",
    "JSON Validity: Is the assessment a valid JSON matching the required schema? (Assume basic validity if parsed).\n",
    "\n",
    "Output:\n",
    "If the AI's Assessment is PERFECT and strictly adheres to all rules: Respond ONLY with the word ACCEPT.\n",
    "If there are ANY flaws (rule misapplication, poor reasoning, incorrect reformulation, wrong null handling): Provide a concise, actionable critique (1-3 sentences) explaining EXACTLY what needs to be fixed. Focus on the most critical error if multiple exist. Do NOT provide the corrected JSON yourself.\"\"\"),\n",
    "(\"human\", \"Evaluate the AI Verifier's Assessment based on the rules and context. Provide your critique or 'ACCEPT'.\")\n",
    "])\n",
    "\n",
    "\n",
    "REVISER_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"\"\"You are an AI Data Quality Analyst revising your previous assessment based on critique.\n",
    "\n",
    "Original Task & Rules (Reminder):\n",
    "Verify if the Product Information makes the product an \"Exact\" match for the Search Query based on the rules below. Output valid JSON.\n",
    "Definition Exact ('E'): Relevant AND satisfies ALL query specs.\n",
    "Rule 1 (Contradiction): Explicit contradiction -> is_exact_match: false.\n",
    "Rule 2 (Missing Info): Spec not mentioned -> Assume OK (is_exact_match: true unless Rule 1 applies).\n",
    "Rule 3 (Extra Info): Extra product details OK if no contradiction.\n",
    "Context:\n",
    "Search Query: \"{query}\"\n",
    "Product Information:\n",
    "--- START ---\n",
    "{product_context}\n",
    "--- END ---\n",
    "Your Previous Assessment (JSON):\n",
    "{previous_assessment_json}\n",
    "\n",
    "Critique Received:\n",
    "\"{critique}\"\n",
    "\n",
    "Your Task:\n",
    "Carefully consider the Critique. Re-evaluate the Query and Product Information based only on the original Rules and the specific points raised in the Critique.\n",
    "Generate a revised, valid JSON object adhering to the VerificationResult schema, addressing the critique accurately. Ensure reformulated_query is null if is_exact_match is true.\"\"\"),\n",
    "(\"human\", \"Revise your assessment based on the critique and rules. Provide the updated JSON output.\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "ddLJcQPtrG2u",
    "outputId": "e9cb14d0-7f5c-4677-ea54-cb22f7127926"
   },
   "outputs": [],
   "source": [
    "# --- Bind the GENERATOR LLM for structured output ---\n",
    "#  generator_llm is defined from the setup block\n",
    "structured_llm_generator = generator_llm.with_structured_output(VerificationResult)\n",
    "\n",
    "print(\"Core Components Defined (Pydantic Model, Prompts, Structured LLM binding).\")\n",
    "\n",
    "\n",
    "# --- LangGraph State Definition ---\n",
    "# The state defines the data that flows through the graph.\n",
    "# Each node updates parts of this state.\n",
    "class VerificationState(TypedDict):\n",
    "    query: str                      # Input: The user's search query\n",
    "    product_context: str            # Input: The aggregated product information\n",
    "    assessment: Optional[VerificationResult] # Output of verify/revise: The current JSON assessment\n",
    "    critique: Optional[str]         # Output of reflect: The critique text or \"ACCEPT\"\n",
    "    revision_number: int            # Internal counter for reflection cycles\n",
    "    error: Optional[str]            # Captures errors from nodes\n",
    "\n",
    "\n",
    "# --- Graph Node Definitions ---\n",
    "# Nodes are functions that perform actions and update the state.\n",
    "\n",
    "def initial_verify(state: VerificationState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node: Performs the first verification attempt.\n",
    "    Input: query, product_context\n",
    "    Output: assessment, error, revision_number (incremented)\n",
    "    LLM Used: Generator (structured output)\n",
    "    \"\"\"\n",
    "    print(f\"--- Iteration {state['revision_number']}: Initial Verification (using {GENERATOR_MODEL_NAME}) ---\")\n",
    "    query = state['query']\n",
    "    product_context = state['product_context']\n",
    "    error = None\n",
    "    assessment = None\n",
    "    try:\n",
    "        prompt = VERIFIER_PROMPT_TEMPLATE.format(query=query, product_context=product_context)\n",
    "        assessment = structured_llm_generator.invoke(prompt) # Call LLM for structured JSON\n",
    "        print(f\"Initial Assessment: {assessment.json(indent=2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in initial_verify: {e}\")\n",
    "        error = f\"Failed during initial verification: {e}\"\n",
    "    # Increment revision number for the next potential cycle\n",
    "    return {\"assessment\": assessment, \"error\": error, \"revision_number\": state.get(\"revision_number\", 0) + 1}\n",
    "\n",
    "def reflect(state: VerificationState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node: Critiques the current assessment.\n",
    "    Input: query, product_context, assessment, error (from previous step)\n",
    "    Output: critique, error\n",
    "    LLM Used: Critic\n",
    "    \"\"\"\n",
    "    current_revision = state.get(\"revision_number\", 1) # Revision number *after* verify/revise\n",
    "    print(f\"--- Iteration {current_revision - 1}: Reflection (using {CRITIC_MODEL_NAME}) ---\")\n",
    "    if state.get(\"error\"): # If verify/revise failed, skip reflection\n",
    "        print(\"Skipping reflection due to previous error.\")\n",
    "        return {\"critique\": \"Previous step failed\", \"error\": state.get(\"error\")}\n",
    "\n",
    "    query = state['query']\n",
    "    product_context = state['product_context']\n",
    "    assessment = state['assessment']\n",
    "    critique = None\n",
    "    error = None\n",
    "\n",
    "    if not assessment: # Should not happen if verify worked, but check defensively\n",
    "        error = \"Cannot reflect: No assessment provided from previous step.\"\n",
    "        print(error)\n",
    "        return {\"critique\": None, \"error\": error}\n",
    "\n",
    "    try:\n",
    "        assessment_json_str = assessment.json()\n",
    "        prompt = REFLECTOR_PROMPT_TEMPLATE.format(\n",
    "            query=query,\n",
    "            product_context=product_context,\n",
    "            assessment_json=assessment_json_str\n",
    "        )\n",
    "        response = critic_llm.invoke(prompt) # Use the critic LLM\n",
    "        critique = response.content.strip()\n",
    "        print(f\"Reflection Critique: {critique}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in reflect: {e}\")\n",
    "        error = f\"Failed during reflection: {e}\"\n",
    "\n",
    "    # Standardize the acceptance signal\n",
    "    if critique and critique.upper() == \"ACCEPT\":\n",
    "        critique = \"ACCEPT\"\n",
    "\n",
    "    # Return the critique; revision number doesn't change in this step's output\n",
    "    return {\"critique\": critique, \"error\": error}\n",
    "\n",
    "\n",
    "def revise(state: VerificationState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node: Revises the assessment based on the critique.\n",
    "    Input: query, product_context, assessment, critique, revision_number, error\n",
    "    Output: assessment (updated), error, revision_number (incremented)\n",
    "    LLM Used: Generator (structured output)\n",
    "    \"\"\"\n",
    "    current_revision = state.get(\"revision_number\", 1) # Revision number *before* this revision\n",
    "    print(f\"--- Iteration {current_revision - 1}: Revision (using {GENERATOR_MODEL_NAME}) ---\")\n",
    "    if state.get(\"error\"): # If reflection failed, skip revision\n",
    "        print(\"Skipping revision due to previous error.\")\n",
    "        # Pass along previous state's assessment and error\n",
    "        return {\"assessment\": state.get(\"assessment\"), \"error\": state.get(\"error\"), \"revision_number\": current_revision}\n",
    "\n",
    "    query = state['query']\n",
    "    product_context = state['product_context']\n",
    "    previous_assessment = state['assessment']\n",
    "    critique = state['critique']\n",
    "    revised_assessment = None\n",
    "    error = None\n",
    "\n",
    "    # This node should only run if critique is NOT \"ACCEPT\"\n",
    "    if not previous_assessment or not critique or critique == \"ACCEPT\":\n",
    "        error = \"Revise node called inappropriately (no assessment/critique or critique was ACCEPT).\"\n",
    "        print(f\"WARNING: {error}\")\n",
    "        # Return previous state without incrementing revision number if called incorrectly\n",
    "        return {\"assessment\": previous_assessment, \"error\": error, \"revision_number\": current_revision}\n",
    "\n",
    "    try:\n",
    "        previous_assessment_json_str = previous_assessment.json()\n",
    "        prompt = REVISER_PROMPT_TEMPLATE.format(\n",
    "            query=query,\n",
    "            product_context=product_context,\n",
    "            previous_assessment_json=previous_assessment_json_str,\n",
    "            critique=critique\n",
    "        )\n",
    "        revised_assessment = structured_llm_generator.invoke(prompt) # Use generator for structured output\n",
    "        print(f\"Revised Assessment: {revised_assessment.json(indent=2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in revise: {e}\")\n",
    "        error = f\"Failed during revision: {e}\"\n",
    "        # Keep the previous assessment if revision fails to avoid losing progress\n",
    "        revised_assessment = previous_assessment\n",
    "\n",
    "    # Return the revised assessment and increment revision number for the next cycle\n",
    "    return {\"assessment\": revised_assessment, \"error\": error, \"revision_number\": current_revision + 1}\n",
    "\n",
    "\n",
    "# --- Conditional Edge Logic ---\n",
    "# This function directs the flow of the graph after the 'reflect' node runs.\n",
    "def decide_to_finish(state: VerificationState) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next step after reflection: revise or end the process.\n",
    "    Input: The current state after the 'reflect' node.\n",
    "    Output: A string representing the next node ('revise') or the end state (END).\n",
    "    \"\"\"\n",
    "    error = state.get(\"error\")\n",
    "    critique = state.get(\"critique\")\n",
    "    # revision_number reflects the *upcoming* cycle attempt number (e.g., 1 after initial verify)\n",
    "    revision_number = state.get(\"revision_number\", 0)\n",
    "\n",
    "    print(f\"Decision Check: Error='{error}', Critique='{critique}', Revision Cycle Attempt='{revision_number}'\")\n",
    "\n",
    "    # Prioritize ending on critical errors from verify or reflect\n",
    "    is_critical_error = error and \"Revise node called inappropriately\" not in error\n",
    "\n",
    "    if is_critical_error:\n",
    "        print(f\"Deciding to finish due to critical error: {error}\")\n",
    "        return END # Stop the graph execution for this item\n",
    "    elif critique == \"ACCEPT\":\n",
    "        print(\"Deciding to finish: Reflection accepted.\")\n",
    "        return END # Stop the graph, the assessment is good\n",
    "    elif revision_number > MAX_ITERATIONS: # Have we exceeded the allowed cycles\n",
    "        print(f\"Deciding to finish: Max iterations ({MAX_ITERATIONS}) reached.\")\n",
    "        return END # Stop the graph due to hitting the limit\n",
    "    else:\n",
    "        # If no error, not accepted, and within iteration limit, proceed to revise\n",
    "        print(\"Deciding to revise.\")\n",
    "        return \"revise\" # Route to the 'revise' node\n",
    "\n",
    "# --- Build the Graph ---\n",
    "# building the reflection logicusing LangGraph's core components.\n",
    "# gives us more control over the specific flow and state management.\n",
    "\n",
    "workflow = StateGraph(VerificationState)\n",
    "\n",
    "# Add the defined functions as nodes in the graph\n",
    "workflow.add_node(\"verify\", initial_verify)\n",
    "workflow.add_node(\"reflect\", reflect)\n",
    "workflow.add_node(\"revise\", revise)\n",
    "\n",
    "# Define the starting point of the graph\n",
    "workflow.set_entry_point(\"verify\")\n",
    "\n",
    "# Define the connections (edges) between nodes\n",
    "workflow.add_edge(\"verify\", \"reflect\") # Always go from initial verification to reflection\n",
    "\n",
    "# Add conditional logic after the 'reflect' node\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflect\",          # The source node\n",
    "    decide_to_finish,   # The function that decides the next step\n",
    "    {\n",
    "        \"revise\": \"revise\", # If decide_to_finish returns \"revise\", go to the 'revise' node\n",
    "        END: END            # If decide_to_finish returns END, terminate the graph for this run\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the loop: after revising, go back to reflect on the revision\n",
    "workflow.add_edge(\"revise\", \"reflect\")\n",
    "\n",
    "# Compile the graph into a runnable application object\n",
    "app = workflow.compile()\n",
    "print(\"LangGraph Application Compiled.\")\n",
    "\n",
    "# --- Visualize the graph ---\n",
    "print(\"\\nAttempting to visualize the graph...\")\n",
    "try:\n",
    "    img_data = app.get_graph().draw_mermaid_png() # Generate the graph image\n",
    "    if img_data:\n",
    "        print(\"Displaying Mermaid PNG:\")\n",
    "        display(Image(img_data)) # Display in the notebook\n",
    "    else:\n",
    "        # Fallback if PNG generation fails but no error is raised\n",
    "        print(\"draw_mermaid_png() did not return image data. Graphviz might have issues.\")\n",
    "        print(\"Attempting fallback with draw_ascii()...\")\n",
    "        try:\n",
    "            ascii_graph = app.get_graph().draw_ascii()\n",
    "            print(\"\\nASCII Representation of the Graph:\")\n",
    "            print(ascii_graph)\n",
    "        except Exception as ascii_e:\n",
    "            print(f\"Could not draw ASCII graph either: {ascii_e}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    # Catch if pygraphviz is missing\n",
    "    print(f\"ImportError: {e}. Visualization requires pygraphviz and graphviz system library.\")\n",
    "    print(\"Install with: !apt-get install -qq graphviz libgraphviz-dev && pip install -q pygraphviz\")\n",
    "except Exception as e:\n",
    "    # Catch other errors during visualization\n",
    "    print(f\"Could not draw graph: {e}\")\n",
    "    print(\"Attempting fallback with draw_ascii()...\")\n",
    "    try:\n",
    "        ascii_graph = app.get_graph().draw_ascii()\n",
    "        print(\"\\nASCII Representation of the Graph:\")\n",
    "        print(ascii_graph)\n",
    "    except Exception as ascii_e:\n",
    "        print(f\"Could not draw ASCII graph either: {ascii_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ceba958f8bc64f6a9a652ff7919762b9",
      "340534e2773f4761900e8de3e7681e42",
      "816465ce9ff3478b87f3debb8b4903ce",
      "0409aa4938b645b98b0e4822e4d187cf",
      "ddc147cbc71c4455a1c90871ba6edb2a",
      "c0dc1b526d45417cadeb658fc0014e39",
      "5b8adbb383414d04a1e63d4cd85a9ace",
      "0779ff545942461ab7024ad015cd3c3e",
      "479616ab43094cf195a09adff62a69f3",
      "043e596aae604f7cb85efe469697fade",
      "78381d28c4064ccebc884b55354f1a81"
     ]
    },
    "id": "JXaCjd7nrMpY",
    "outputId": "aaa8896c-041b-4599-e709-0e2b91e9dbe9"
   },
   "outputs": [],
   "source": [
    "# --- Run the Workflow for Each Row ---\n",
    "results_list = []\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "print(f\"\\nStarting Reflection Agent Workflow for {len(df_input)} items...\")\n",
    "\n",
    "start_overall_time = time.time()\n",
    "\n",
    "# Iterate through each row of the prepared DataFrame\n",
    "for index, row in tqdm(df_input.iterrows(), total=len(df_input), desc=\"Processing Items\"):\n",
    "    query_id = row['query_id']\n",
    "    product_id = row['product_id']\n",
    "    query = row['query']\n",
    "    # Handle potential missing context robustly\n",
    "    context = row.get('llm_product_context', \"Product information missing.\")\n",
    "    if pd.isna(context) or not context:\n",
    "         context = \"Product information missing.\"\n",
    "\n",
    "    print(f\"\\n===== Processing Item {index+1}/{len(df_input)} (PID: {product_id}) =====\")\n",
    "    # Define the initial state for this item's graph run\n",
    "    initial_state = VerificationState(\n",
    "        query=query,\n",
    "        product_context=context,\n",
    "        assessment=None,\n",
    "        critique=None,\n",
    "        revision_number=0, # Start revision count at 0 (will become 1 after initial_verify)\n",
    "        error=None,\n",
    "    )\n",
    "\n",
    "    final_state = None\n",
    "    # Initialize a dictionary to store the results for this specific item\n",
    "    item_result = {\n",
    "        'query_id': query_id,\n",
    "        'product_id': product_id,\n",
    "        'accurate_label': None, # Default to None (will be updated based on final assessment)\n",
    "        'reformulated_query': None, # Default to None\n",
    "        'final_reasoning': \"Workflow did not produce a final assessment.\", # Default message\n",
    "        'iterations': 0, # Number of revision cycles (0 means only initial verify + reflect)\n",
    "        'final_outcome': \"Unknown\" # Tracks how the graph ended (Accepted, Error, Max Iterations)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Execute the compiled LangGraph application for this item's initial state\n",
    "        # The recursion_limit prevents infinite loops if something goes wrong\n",
    "        final_state = app.invoke(initial_state, {\"recursion_limit\": MAX_ITERATIONS * 2 + 5})\n",
    "\n",
    "        # Process the final state returned by the graph after it reaches END\n",
    "        if final_state:\n",
    "            # Calculate the number of revision cycles completed\n",
    "            # revision_number in the final state is the number of the *next* cycle attempt\n",
    "            item_result['iterations'] = final_state.get('revision_number', 1) - 1 # Iteration 0 is initial verify\n",
    "\n",
    "            # Extract key information from the final state\n",
    "            final_assessment = final_state.get('assessment')\n",
    "            final_critique = final_state.get('critique') # The critique that led to the END state\n",
    "            final_error = final_state.get('error') # Any error that occurred\n",
    "\n",
    "            # Determine the final outcome and extract results based on how the graph ended\n",
    "            is_critical_error = final_error and \"Revise node called inappropriately\" not in final_error\n",
    "            max_iters_reached = item_result['iterations'] >= MAX_ITERATIONS\n",
    "\n",
    "            if is_critical_error:\n",
    "                item_result['final_outcome'] = \"Error\"\n",
    "                item_result['final_reasoning'] = f\"Workflow ended with error: {final_error}\"\n",
    "                # accurate_label remains None\n",
    "            elif final_critique == \"ACCEPT\":\n",
    "                item_result['final_outcome'] = \"Accepted\"\n",
    "                # Extract results from the accepted assessment\n",
    "                if final_assessment and isinstance(final_assessment, VerificationResult):\n",
    "                    item_result['accurate_label'] = final_assessment.is_exact_match\n",
    "                    item_result['reformulated_query'] = final_assessment.reformulated_query\n",
    "                    item_result['final_reasoning'] = final_assessment.reasoning\n",
    "                else:\n",
    "                     # If accepted but assessment is somehow invalid, flag it\n",
    "                     item_result['final_reasoning'] = \"Accepted, but final assessment missing or invalid.\"\n",
    "                     item_result['accurate_label'] = None # Mark as undetermined\n",
    "            elif max_iters_reached:\n",
    "                item_result['final_outcome'] = \"Max Iterations Reached\"\n",
    "                # Take the last assessment available when max iterations is hit\n",
    "                if final_assessment and isinstance(final_assessment, VerificationResult):\n",
    "                    item_result['accurate_label'] = final_assessment.is_exact_match\n",
    "                    item_result['reformulated_query'] = final_assessment.reformulated_query\n",
    "                    item_result['final_reasoning'] = final_assessment.reasoning + \" [Note: Max iterations reached]\"\n",
    "                else:\n",
    "                     item_result['final_reasoning'] = \"Max iterations reached, but final assessment missing or invalid.\"\n",
    "                     item_result['accurate_label'] = None # Mark as undetermined\n",
    "            else:\n",
    "                # Defensive case: Graph ended without ACCEPT, Error, or Max Iterations\n",
    "                item_result['final_outcome'] = \"Ended Unexpectedly\"\n",
    "                item_result['final_reasoning'] = f\"Ended unexpectedly. Last critique: {final_critique}. Last Error: {final_error}\"\n",
    "                item_result['accurate_label'] = None # Mark as undetermined\n",
    "\n",
    "        else:\n",
    "            # If app.invoke returns None (shouldn't typically happen unless graph is ill-defined)\n",
    "            item_result['final_outcome'] = \"Error\"\n",
    "            item_result['final_reasoning'] = \"Graph invocation returned None.\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch errors occurring outside the graph execution itself (e.g., during setup for the loop)\n",
    "        print(f\"\\nFATAL ERROR processing item index {index} (PID: {product_id}): {e}\")\n",
    "        item_result['final_reasoning'] = f\"Outer loop error: {e}\"\n",
    "        item_result['final_outcome'] = \"Outer Loop Error\"\n",
    "        item_result['accurate_label'] = None # Ensure label is None on outer error\n",
    "        # Optional: Add traceback for debugging\n",
    "        # import traceback\n",
    "        # item_result['error_trace'] = traceback.format_exc()\n",
    "\n",
    "    # Final check to ensure accurate_label is valid (boolean or None/NA)\n",
    "    if item_result['accurate_label'] not in [True, False, None, pd.NA]:\n",
    "         warnings.warn(f\"Item {index} resulted in non-boolean/None accurate_label: {item_result['accurate_label']}. Setting to None.\")\n",
    "         item_result['accurate_label'] = None\n",
    "\n",
    "\n",
    "    # Append the processed result for this item to the list\n",
    "    results_list.append(item_result)\n",
    "    print(f\"===== Finished Item {index+1} (PID: {product_id}) - Outcome: {item_result['final_outcome']} =====\")\n",
    "\n",
    "\n",
    "end_overall_time = time.time()\n",
    "print(f\"\\nReflection Agent Workflow Completed in {end_overall_time - start_overall_time:.2f} seconds.\")\n",
    "\n",
    "# --- Process and Display Results ---\n",
    "\n",
    "# Convert the list of result dictionaries into a pandas DataFrame\n",
    "df_results_reflection = pd.DataFrame(results_list)\n",
    "\n",
    "# Explicitly set dtype for boolean column to handle potential NAs correctly (uses pandas nullable boolean type)\n",
    "df_results_reflection['accurate_label'] = df_results_reflection['accurate_label'].astype('boolean')\n",
    "\n",
    "print(\"\\n--- Sample Results (Reflection Agent - Multi-Model) ---\")\n",
    "# Display the first few rows of the results DataFrame\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_colwidth', 150):\n",
    "    display(df_results_reflection.head())\n",
    "\n",
    "# --- Basic Statistics ---\n",
    "print(\"\\n--- Basic Stats ---\")\n",
    "print(f\"Total items processed: {len(df_results_reflection)}\")\n",
    "\n",
    "# Analyze the distribution of the final 'accurate_label'\n",
    "if 'accurate_label' in df_results_reflection.columns:\n",
    "    print(\"\\nFinal Label Distribution:\")\n",
    "    # Use value_counts with dropna=False to include undetermined cases (None/pd.NA)\n",
    "    label_counts = df_results_reflection['accurate_label'].value_counts(dropna=False).rename({True: 'Accurate', False: 'Inaccurate'})\n",
    "    # Rename the index for NaN/NA to be more descriptive\n",
    "    if pd.isna(label_counts.index).any():\n",
    "        rename_map = {idx: 'Undetermined (Error/MaxIter)' for idx in label_counts.index if pd.isna(idx)}\n",
    "        label_counts = label_counts.rename(index=rename_map)\n",
    "    print(label_counts)\n",
    "else:\n",
    "    print(\"Accurate label column not found.\")\n",
    "\n",
    "# Analyze how many revision iterations were needed\n",
    "if 'iterations' in df_results_reflection.columns:\n",
    "    print(\"\\nIterations Distribution (0 = initial verify only):\")\n",
    "    print(df_results_reflection['iterations'].value_counts().sort_index())\n",
    "\n",
    "# Analyze how the graph execution ended for each item\n",
    "if 'final_outcome' in df_results_reflection.columns:\n",
    "    print(\"\\nFinal Outcome Distribution:\")\n",
    "    print(df_results_reflection['final_outcome'].value_counts())\n",
    "\n",
    "\n",
    "# --- Save Final Results ---\n",
    "# Define the columns required for the final deliverable CSV\n",
    "submission_columns = ['query_id', 'product_id', 'accurate_label', 'reformulated_query']\n",
    "\n",
    "try:\n",
    "    # Ensure the final DataFrame has the necessary columns\n",
    "    if not all(col in df_results_reflection.columns for col in submission_columns):\n",
    "        missing_sub = [col for col in submission_columns if col not in df_results_reflection.columns]\n",
    "        raise ValueError(f\"Cannot save submission file. Missing columns: {missing_sub}\")\n",
    "\n",
    "    # Create the DataFrame for submission, selecting only required columns described on the PDF document\n",
    "    submission_df = df_results_reflection[submission_columns].copy()\n",
    "\n",
    "    # Clean up for CSV: Ensure reformulated_query is an empty string instead of None/NA\n",
    "    submission_df['reformulated_query'] = submission_df['reformulated_query'].fillna('').astype(str)\n",
    "    # Note: Pandas to_csv usually handles None/pd.NA in boolean columns as empty strings by default.\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(REFLECTION_RESULTS_CSV, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSuccessfully saved reflection agent results to: {REFLECTION_RESULTS_CSV}\")\n",
    "\n",
    "    # let's save also  the full results DataFrame (including iterations, outcome, reasoning) for analysis\n",
    "    full_reflection_results_csv = REFLECTION_RESULTS_CSV.replace(\".csv\", \"_FULL.csv\")\n",
    "    df_results_reflection.to_csv(full_reflection_results_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Successfully saved full reflection agent results to: {full_reflection_results_csv}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR saving reflection results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H5lhgmI7riLO",
    "outputId": "8863d9c4-1094-4255-cf2a-47046b08fcd3"
   },
   "outputs": [],
   "source": [
    "# --- Executive Summary Cell ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \" Executive Summary Generation (Reflection Agent) \" + \"=\"*30)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# These variables should be available from the previous cell's execution\n",
    "required_data_vars = {\n",
    "    'df_results_reflection': 'df_results_reflection', # DataFrame with results\n",
    "    'GENERATOR_MODEL_NAME': 'GENERATOR_MODEL_NAME', # Name of the generator LLM\n",
    "    'CRITIC_MODEL_NAME': 'CRITIC_MODEL_NAME',       # Name of the critic LLM\n",
    "    'REFLECTION_RESULTS_CSV': 'REFLECTION_RESULTS_CSV', # Output filename\n",
    "    'MAX_ITERATIONS': 'MAX_ITERATIONS'              # Max reflection cycles\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for display_name, actual_name in required_data_vars.items():\n",
    "    if actual_name not in locals():\n",
    "        missing_vars.append(f\"'{actual_name}' (needed for {display_name})\")\n",
    "\n",
    "# --- Proceed only if essential variables exist ---\n",
    "if missing_vars:\n",
    "     summary_md = f\"## LLM Verification Summary (Reflection Agent)\\n\\nERROR: Could not generate summary. Essential analysis data missing:\\n- {chr(10).join(missing_vars)}\"\n",
    "     warnings.warn(f\"Missing variables needed for summary: {', '.join(missing_vars)}\")\n",
    "     display(Markdown(summary_md)) # Display the error message\n",
    "else:\n",
    "    # --- Prepare data for summary using variables verified to exist ---\n",
    "    df_results = locals()[required_data_vars['df_results_reflection']]\n",
    "    generator_model = locals()[required_data_vars['GENERATOR_MODEL_NAME']]\n",
    "    critic_model = locals()[required_data_vars['CRITIC_MODEL_NAME']]\n",
    "    final_csv = locals()[required_data_vars['REFLECTION_RESULTS_CSV']]\n",
    "    max_iterations = locals()[required_data_vars['MAX_ITERATIONS']]\n",
    "    full_csv = final_csv.replace(\".csv\", \"_FULL.csv\") # Derive full CSV name\n",
    "\n",
    "    total_items = len(df_results)\n",
    "\n",
    "    # Calculate final label distribution from the results DataFrame\n",
    "    final_label_counts = df_results['accurate_label'].value_counts(dropna=False).rename({True: 'Accurate', False: 'Inaccurate'})\n",
    "    if pd.isna(final_label_counts.index).any():\n",
    "        rename_map = {idx: 'Undetermined (Error/MaxIter)' for idx in final_label_counts.index if pd.isna(idx)}\n",
    "        final_label_counts = final_label_counts.rename(index=rename_map)\n",
    "\n",
    "    num_accurate = final_label_counts.get('Accurate', 0)\n",
    "    num_inaccurate = final_label_counts.get('Inaccurate', 0)\n",
    "    num_undetermined = final_label_counts.get('Undetermined (Error/MaxIter)', 0)\n",
    "\n",
    "    # Get outcome distribution\n",
    "    outcome_counts = df_results['final_outcome'].value_counts()\n",
    "    num_accepted = outcome_counts.get('Accepted', 0)\n",
    "    num_max_iter = outcome_counts.get('Max Iterations Reached', 0)\n",
    "    num_errors = outcome_counts.get('Error', 0) + outcome_counts.get('Outer Loop Error', 0) + outcome_counts.get('Ended Unexpectedly', 0)\n",
    "\n",
    "\n",
    "    # Helper function to calculate and format percentage string safely\n",
    "    def format_percentage_string(count, total):\n",
    "        \"\"\"Calculates percentage and formats to string 'XX.X%', or returns 'N/A'.\"\"\"\n",
    "        if isinstance(count, int) and isinstance(total, int) and total > 0:\n",
    "            percentage_value = (count / total)\n",
    "            return f\"{percentage_value:.1%}\"\n",
    "        else:\n",
    "            return \"N/A\"\n",
    "\n",
    "    # Calculate percentages\n",
    "    perc_accurate = format_percentage_string(num_accurate, total_items)\n",
    "    perc_inaccurate = format_percentage_string(num_inaccurate, total_items)\n",
    "    perc_undetermined = format_percentage_string(num_undetermined, total_items)\n",
    "\n",
    "    # --- Construct Markdown Summary ---\n",
    "    summary_md = f\"\"\"\n",
    "## LLM Verification Task: Executive Summary (Reflection Agent Approach)\n",
    "\n",
    "**Objective:** Verify the accuracy of the 'Exact' (E) label for **{total_items}** specific query-product pairs using a single LLM with a reflection/critique loop. Reformulate queries where the 'E' label was deemed inaccurate due to explicit contradictions.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Data Preparation:** Loaded ESCI data, merged product details, filtered for 3 target queries with 'E' labels (`aa batteries 100 pack`, `kodak photo paper...`, `dewalt 8v...`).\n",
    "2.  **Reflection Agent (LangGraph):**\n",
    "    *   An agent loop was implemented using LangGraph.\n",
    "    *   **Verifier/Reviser LLM ({generator_model}):** Generated initial JSON assessments and revised them based on critique, adhering to strict rules (Contradiction, Missing Info, Extra Info).\n",
    "    *   **Critic LLM ({critic_model}):** Evaluated the Verifier/Reviser's JSON output against the rules. It responded with \"ACCEPT\" if correct or provided concise critique otherwise.\n",
    "    *   **Loop:** The process iterated (verify -> reflect -> revise -> reflect...) until the Critic outputted \"ACCEPT\" or the maximum of **{max_iterations}** revision cycles was reached.\n",
    "3.  **Output:** The final assessment from the loop (either accepted or the last one before max iterations) was recorded.\n",
    "\n",
    "**Key Findings & Metrics:**\n",
    "\n",
    "*   **Total Items Analyzed:** {total_items}\n",
    "*   **Final Label Distribution (After Reflection):**\n",
    "    *   ✅ **Accurate:** **{num_accurate}** ({perc_accurate}) - *Final assessment confirmed 'E' label.*\n",
    "    *   ❌ **Inaccurate:** **{num_inaccurate}** ({perc_inaccurate}) - *Final assessment deemed 'E' label incorrect due to contradictions.*\n",
    "    *   ❓ **Undetermined:** **{num_undetermined}** ({perc_undetermined}) - *Workflow ended due to max iterations or error before an 'ACCEPT' state.*\n",
    "*   **Workflow Outcomes:**\n",
    "    *   **Accepted:** {num_accepted} items reached an \"ACCEPT\" state from the critic.\n",
    "    *   **Max Iterations Reached:** {num_max_iter} item hit the revision limit.\n",
    "    *   **Errors:** {num_errors} items encountered errors during processing.\n",
    "*   **Efficiency:** Most items ({outcome_counts.get('Accepted', 0)} out of {total_items}) were accepted after 0 or 1 revision cycles, indicating good initial accuracy from the generator LLM ({generator_model}).\n",
    "\n",
    "**Analysis Highlights:**\n",
    "\n",
    "1.  **Inaccurate Labels Identified:** The **{num_inaccurate}** items ultimately marked 'Inaccurate' represent clear instances where product details explicitly contradicted the search query. The types of contradictions were consistent with the previous multi-model approach (pack size, attribute, voltage, brand, product type). The generated `reformulated_query` in the output file provides suggested corrections.\n",
    "\n",
    "2.  **Self-Correction Demonstrated:** The reflection mechanism proved effective. For instance, item 6 (PID: `B00KMDL8U6`) was initially assessed as `False` (Inaccurate) due to conflicting counts in title vs. bullets. The critic correctly pointed out the title precedence, leading the reviser to correct the assessment to `True` (Accurate), which was then accepted. This highlights the pattern's ability to catch and fix initial LLM errors based on predefined rules.\n",
    "\n",
    "3.  **Loop Termination:** The maximum iteration limit functioned as intended. Item 9 (PID: `B07TWK2S22`) reached the limit after {max_iterations} revisions. While the critic still found flaws in the final reformulated query (incorrectly including \"gyroscopic\"), the loop terminated, providing the *last available assessment* rather than looping indefinitely. This demonstrates controlled execution but also shows that complex critiques might require more iterations or refined prompting.\n",
    "\n",
    "4.  **Comparison to Ensemble:** This reflection approach yielded the **same final `accurate_label` distribution (16 Accurate, 8 Inaccurate)** as the 3-model ensemble method for this specific dataset. This suggests that for this task complexity and these models, both robust methods converged on the same outcome, though potentially via different paths (self-correction vs. voting).\n",
    "\n",
    "**Benefits & Considerations of Reflection Agent:**\n",
    "\n",
    "*   **Self-Correction:** The primary benefit is the potential to improve accuracy by catching and correcting the generator's own mistakes, as seen with item 6. This can be crucial for high-stakes tasks.\n",
    "*   **Resource Efficiency (Model Count):** Requires managing only one primary generator and one (potentially smaller/faster) critic model, compared to multiple large models in the ensemble.\n",
    "*   **Latency/Cost per Item:** Can be higher than a single LLM call due to multiple sequential calls (verify, reflect, potentially revise+reflect). The total cost depends heavily on the number of iterations needed per item.\n",
    "*   **Prompt Complexity:** Requires careful crafting of not just the generator prompt but also the critic prompt to ensure accurate and useful feedback.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "*   **Action Inaccurate Findings:**   We should prioritize reviewing the **{num_inaccurate}** items flagged as 'Inaccurate'. Evaluate the LLM-generated `reformulated_query` (available in `{final_csv}`) for potential catalog or search relevance improvements.\n",
    "*   **Leverage High Confidence Results:** The **{num_accurate}** items confirmed as 'Accurate' (most accepted after 0 revisions) represent high-confidence results suitable for streamlined review.\n",
    "*   **Consider Use Cases:** The reflection pattern is particularly valuable when:\n",
    "    *   Initial errors are costly or unacceptable.\n",
    "    *   A mechanism for rule-based refinement is needed.\n",
    "    *   Running multiple large models simultaneously is resource-prohibitive, but slightly higher latency per item is acceptable.\n",
    "*   **Future Tuning:** Experiment with different critic models, adjust `MAX_ITERATIONS`, or refine the critic prompt for more nuanced feedback, especially for complex cases like item 9's reformulation.\n",
    "\n",
    "**Output:** The final results table required by the exercise (containing `query_id`, `product_id`, `accurate_label`, `reformulated_query`) is saved in `{final_csv}`. A full table including detailed final reasoning, iterations, and outcomes is available in `{full_csv}` for deeper analysis.\n",
    "\"\"\"\n",
    "\n",
    "# --- Display the Summary ---\n",
    "display(Markdown(summary_md))\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0409aa4938b645b98b0e4822e4d187cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_043e596aae604f7cb85efe469697fade",
      "placeholder": "​",
      "style": "IPY_MODEL_78381d28c4064ccebc884b55354f1a81",
      "value": " 24/24 [01:52&lt;00:00,  3.75s/it]"
     }
    },
    "043e596aae604f7cb85efe469697fade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0779ff545942461ab7024ad015cd3c3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "340534e2773f4761900e8de3e7681e42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0dc1b526d45417cadeb658fc0014e39",
      "placeholder": "​",
      "style": "IPY_MODEL_5b8adbb383414d04a1e63d4cd85a9ace",
      "value": "Processing Items: 100%"
     }
    },
    "479616ab43094cf195a09adff62a69f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b8adbb383414d04a1e63d4cd85a9ace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78381d28c4064ccebc884b55354f1a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "816465ce9ff3478b87f3debb8b4903ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0779ff545942461ab7024ad015cd3c3e",
      "max": 24,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_479616ab43094cf195a09adff62a69f3",
      "value": 24
     }
    },
    "c0dc1b526d45417cadeb658fc0014e39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ceba958f8bc64f6a9a652ff7919762b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_340534e2773f4761900e8de3e7681e42",
       "IPY_MODEL_816465ce9ff3478b87f3debb8b4903ce",
       "IPY_MODEL_0409aa4938b645b98b0e4822e4d187cf"
      ],
      "layout": "IPY_MODEL_ddc147cbc71c4455a1c90871ba6edb2a"
     }
    },
    "ddc147cbc71c4455a1c90871ba6edb2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
